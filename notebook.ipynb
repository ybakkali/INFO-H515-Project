{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# INTRODUCTION\n",
    "In this notebook we will be using the following packages as bullet list:\n",
    "  * [pyspark](https://spark.apache.org/docs/latest/api/python/pyspark.html)\n",
    "  * [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
    "  * [pyarrow](https://arrow.apache.org/docs/python/index.html)\n",
    "\n",
    "We will be using the following data:\n",
    "  * dressipi_recsys2022\n",
    "\n",
    "The dataset is split in 4 csv files :\n",
    "   * candidate_items : contains the candidate items for each user\n",
    "   * item_features : contains the features for each item (items can have multiple features)\n",
    "   * train_purchases : contains the purchases for each user\n",
    "   * train_sessions : contains the sessions for each user\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Using cached pyarrow-8.0.0-cp39-cp39-win_amd64.whl (17.9 MB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\natha\\appdata\\local\\programs\\python\\python39\\undclimo\\lib\\site-packages (from pyarrow) (1.20.2)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-8.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python39\\UndCliMo\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to install the required packages\n",
    "# %pip install pyspark\n",
    "# %pip install pyarrow\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate_items: 0\n",
      "item_features: 0\n",
      "train_purchases: 0\n",
      "train_sessions: 0\n",
      "+----------+-------+--------------------+-------+\n",
      "|session_id|item_id|                date|user_id|\n",
      "+----------+-------+--------------------+-------+\n",
      "|         3|   9655|2020-12-18 21:25:...|   9655|\n",
      "|         3|   9655|2020-12-18 21:19:...|   9655|\n",
      "|        13|  15654|2020-03-13 19:35:...|  15654|\n",
      "|        18|  18316|2020-08-26 19:18:...|  18316|\n",
      "|        18|   2507|2020-08-26 19:16:...|   2507|\n",
      "+----------+-------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataset import *\n",
    "from pipeline import *\n",
    "from our_model import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# start spark session\n",
    "spark = SparkSession.builder.appName(\"Python Spark SQL basic example\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "\n",
    "candidate_items, item_features, train_purchases, train_sessions = [None] * 4\n",
    "datasets = [candidate_items, item_features, train_purchases, train_sessions]\n",
    "\n",
    "#load all datasets\n",
    "def load_candidate_items():\n",
    "    global candidate_items\n",
    "    candidate_items = SPARK.read.csv(\"dressipi_recsys2022/candidate_items.csv\", header=True)\n",
    "    candidate_items = candidate_items.withColumn(\"item_id\", candidate_items[\"item_id\"].cast(\"int\"))\n",
    "\n",
    "def load_item_features():\n",
    "    global item_features\n",
    "    item_features = SPARK.read.csv(\"dressipi_recsys2022/item_features.csv\", header=True)\n",
    "    item_features = item_features.withColumn(\"item_id\", item_features[\"item_id\"].cast(\"int\"))\n",
    "    item_features = item_features.withColumn(\"feature_category_id\", item_features[\"feature_category_id\"].cast(\"int\"))\n",
    "    item_features = item_features.withColumn(\"feature_value_id\", item_features[\"feature_value_id\"].cast(\"int\"))\n",
    "\n",
    "def load_train_purchases():\n",
    "    global train_purchases\n",
    "    train_purchases = SPARK.read.csv(\"dressipi_recsys2022/train_purchases.csv\", header=True)\n",
    "    train_purchases = train_purchases.withColumn(\"session_id\", train_purchases[\"session_id\"].cast(\"int\"))\n",
    "    train_purchases = train_purchases.withColumn(\"item_id\", train_purchases[\"item_id\"].cast(\"int\"))\n",
    "    train_purchases = train_purchases.withColumn(\"date\", train_purchases[\"date\"].cast(\"timestamp\"))\n",
    "\n",
    "def load_train_sessions():\n",
    "    global train_sessions\n",
    "    train_sessions = SPARK.read.csv(\"dressipi_recsys2022/train_sessions.csv\", header=True)\n",
    "    train_sessions = train_sessions.withColumn(\"session_id\", train_sessions[\"session_id\"].cast(\"int\"))\n",
    "    train_sessions = train_sessions.withColumn(\"user_id\", train_sessions[\"item_id\"].cast(\"int\"))\n",
    "    train_sessions = train_sessions.withColumn(\"date\", train_sessions[\"date\"].cast(\"timestamp\"))\n",
    "\n",
    "def load_datasets():\n",
    "    global datasets\n",
    "    load_candidate_items()\n",
    "    load_item_features()\n",
    "    load_train_purchases()\n",
    "    load_train_sessions()\n",
    "    datasets = [candidate_items, item_features, train_purchases, train_sessions]\n",
    "\n",
    "load_datasets()\n",
    "\n",
    "# are there missing values (na or null) in any of those dataframes ? print the number of missing values\n",
    "print(\"candidate_items:\", candidate_items.filter(candidate_items[\"item_id\"].isNull()).count())\n",
    "print(\"item_features:\", item_features.filter(item_features[\"item_id\"].isNull()).count())\n",
    "print(\"train_purchases:\", train_purchases.filter(train_purchases[\"item_id\"].isNull()).count())\n",
    "print(\"train_sessions:\", train_sessions.filter(train_sessions[\"item_id\"].isNull()).count())\n",
    "\n",
    "\n",
    "# show the first elements of the dataset\n",
    "train_sessions.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+\n",
      "|item_id|feature_category_id|feature_value_id|\n",
      "+-------+-------------------+----------------+\n",
      "|      2|                 56|             365|\n",
      "|      2|                 62|             801|\n",
      "|      2|                 68|             351|\n",
      "|      2|                 33|             802|\n",
      "|      2|                 72|              75|\n",
      "+-------+-------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_features.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[Row(item_id=2, feature_category_id=56, feature_value_id=365), Row(item_id=2, feature_category_id=62, feature_value_id=801), Row(item_id=2, feature_category_id=68, feature_value_id=351), Row(item_id=2, feature_category_id=33, feature_value_id=802), Row(item_id=2, feature_category_id=72, feature_value_id=75), Row(item_id=2, feature_category_id=29, feature_value_id=123), Row(item_id=2, feature_category_id=16, feature_value_id=38), Row(item_id=2, feature_category_id=50, feature_value_id=76), Row(item_id=2, feature_category_id=61, feature_value_id=462), Row(item_id=2, feature_category_id=53, feature_value_id=6), Row(item_id=2, feature_category_id=7, feature_value_id=394), Row(item_id=2, feature_category_id=69, feature_value_id=885), Row(item_id=2, feature_category_id=47, feature_value_id=123)]\n",
      "[]\n",
      "[Row(session_id=3401213, item_id='2', date=datetime.datetime(2021, 3, 28, 8, 24, 15, 321000), user_id=2)]\n"
     ]
    }
   ],
   "source": [
    "# show rows with item_id == 2 in all dataframes\n",
    "for d in datasets:\n",
    "    print(d.filter(d[\"item_id\"] == 2).collect())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-3732198",
   "language": "python",
   "display_name": "PyCharm (UndCliMo)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}