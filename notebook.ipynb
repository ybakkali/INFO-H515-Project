{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>INFO-H515 - Distributed Data Management and Scalable Analytics</center>\n",
    "\n",
    "## <center>Project 2021-2022</center>\n",
    "\n",
    "#### <center>Bakkali Yahya (000445166)</center>\n",
    "#### <center>Hauwaert Maxime (000461714)</center>\n",
    "#### <center>Marotte Nathan (000459274)</center>\n",
    "\n",
    "### <center>Video URL : https://drive.google.com/file/d/1XA5euCfwmq0Yp1CqiVUHU_7ZTO-Mpx-u/view?usp=sharing</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# INTRODUCTION\n",
    "\n",
    "\n",
    "In this notebook we will be using the following packages:\n",
    "  * [PySpark](https://spark.apache.org/docs/latest/api/python/pyspark.html)\n",
    "  * [Pandas](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
    "  * [Numpy](https://numpy.org/doc/stable/)\n",
    "\n",
    "## Specifications of the problem\n",
    "We are tasked to construct a recommender system as part of the recsyschallenge of 2022, organised by Dressipi, a company focused on providing product and outfit recommendations to leading global retailers.\n",
    "\n",
    "## Dataset\n",
    "The full dataset consists of 1.1 million online retail sessions in the fashion domain, sampled from a 18-month period.\n",
    "\n",
    "It is split in 4 csv files:\n",
    "   * candidate_items.csv : contains the candidate items for the recommender system. This means our model will provide those item_id as output of the prediction.\n",
    "   * item_features.csv : contains the features (for example material, type, colour, etc ... ) for each item, as well as the value of this feature (cotton, skinny, blue, etc ...)\n",
    "   * train_purchases.csv : contains the purchases at the end of each session.\n",
    "   * train_sessions.csv : contains the purchasing sessions, each session is a stream of items viewed, and when an item is purchased, the session ends.\n",
    "\n",
    "Using those 4 files, we will first develop a pipeline to trim it, then engineer the features to regroup different features into one dataset so that it can be used for the recommender system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=16g pyspark-shell\"\n",
    "\n",
    "os.environ['HADOOP_CONF_DIR']=\"/etc/hadoop/conf\"\n",
    "os.environ['PYSPARK_PYTHON']=\"/usr/local/anaconda3/bin/python3\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']=\"/usr/local/anaconda3/bin/python3\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "   .builder \\\n",
    "   .master(\"yarn\") \\\n",
    "   .config(\"spark.executor.instances\",\"5\") \\\n",
    "   .appName(\"group03\") \\\n",
    "   .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "candidate_items, item_features, train_purchases, train_sessions = [None] * 4\n",
    "datasets = [candidate_items, item_features, train_purchases, train_sessions]\n",
    "\n",
    "#load all datasets\n",
    "def load_candidate_items():\n",
    "    global candidate_items\n",
    "    candidate_items = spark.read.csv(\"dressipi_recsys2022/candidate_items.csv\", header=True)\n",
    "    candidate_items = candidate_items.withColumn(\"item_id\", candidate_items[\"item_id\"].cast(\"int\"))\n",
    "\n",
    "def load_item_features():\n",
    "    global item_features\n",
    "    item_features = spark.read.csv(\"dressipi_recsys2022/item_features.csv\", header=True)\n",
    "    item_features = item_features.withColumn(\"item_id\", item_features[\"item_id\"].cast(\"int\"))\n",
    "    item_features = item_features.withColumn(\"feature_category_id\", item_features[\"feature_category_id\"].cast(\"int\"))\n",
    "    item_features = item_features.withColumn(\"feature_value_id\", item_features[\"feature_value_id\"].cast(\"int\"))\n",
    "\n",
    "def load_train_purchases():\n",
    "    global train_purchases\n",
    "    train_purchases = spark.read.csv(\"dressipi_recsys2022/train_purchases.csv\", header=True)\n",
    "    train_purchases = train_purchases.withColumn(\"session_id\", train_purchases[\"session_id\"].cast(\"int\"))\n",
    "    train_purchases = train_purchases.withColumn(\"item_id\", train_purchases[\"item_id\"].cast(\"int\"))\n",
    "    train_purchases = train_purchases.withColumn(\"date\", train_purchases[\"date\"].cast(\"timestamp\"))\n",
    "\n",
    "def load_train_sessions():\n",
    "    global train_sessions\n",
    "    train_sessions = spark.read.csv(\"dressipi_recsys2022/train_sessions.csv\", header=True)\n",
    "    train_sessions = train_sessions.withColumn(\"session_id\", train_sessions[\"session_id\"].cast(\"int\"))\n",
    "    train_sessions = train_sessions.withColumn(\"item_id\", train_sessions[\"item_id\"].cast(\"int\"))\n",
    "    train_sessions = train_sessions.withColumn(\"date\", train_sessions[\"date\"].cast(\"timestamp\"))\n",
    "\n",
    "def load_datasets():\n",
    "    global datasets\n",
    "    load_candidate_items()\n",
    "    load_item_features()\n",
    "    load_train_purchases()\n",
    "    load_train_sessions()\n",
    "    datasets = [candidate_items, item_features, train_purchases, train_sessions]\n",
    "\n",
    "load_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Quick look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### train_sessions.csv\n",
    "This dataset represents the browsing session of a user in the store. It is made of 3 columns:\n",
    "- session_id : the id of the session. It serves as a key to join the data with the other datasets\n",
    "- item_id : the item viewed during the session.\n",
    "- date : the date of at wich the item was viewed.\n",
    "\n",
    "Please note that the browsing sessions end at the end of the day, or if an item was purchased. This means we will not find, for one session, items viewed on 2 different days.\n",
    "Also, as stated on the challenge's website, there are no sessions that do not end with a purchased item in this dataset.\n",
    "\n",
    "Here is a representation of a few of the rows in the train_sessions.csv dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+\n",
      "|session_id|item_id|                date|\n",
      "+----------+-------+--------------------+\n",
      "|         3|   9655|2020-12-18 21:25:...|\n",
      "|         3|   9655|2020-12-18 21:19:...|\n",
      "|        13|  15654|2020-03-13 19:35:...|\n",
      "|        18|  18316|2020-08-26 19:18:...|\n",
      "|        18|   2507|2020-08-26 19:16:...|\n",
      "+----------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "There are 4743820 rows in total.\n"
     ]
    }
   ],
   "source": [
    "train_sessions.show(5)\n",
    "print(f\"There are {train_sessions.count()} rows in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### train_purchases.csv\n",
    "\n",
    "This dataset represents the purchases made by a user in the store. It is made of 3 columns:\n",
    "- session_id : the id of the session.\n",
    "- item_id : the item purchased.\n",
    "- date : the date of at wich the item was purchased.\n",
    "\n",
    "This dataset should be used with the train_sessions dataset to have a more complete picture of the browsing experience of the user. Each session in this dataset ends with an item purchased, noted in the column item_id.\n",
    "\n",
    "Here is a representation of a few of the rows in the train_purchases.csv dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+\n",
      "|session_id|item_id|                date|\n",
      "+----------+-------+--------------------+\n",
      "|         3|  15085|2020-12-18 21:26:...|\n",
      "|        13|  18626|2020-03-13 19:36:...|\n",
      "|        18|  24911|2020-08-26 19:20:...|\n",
      "|        19|  12534|2020-11-02 17:16:...|\n",
      "|        24|  13226|2020-02-26 18:27:...|\n",
      "+----------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "There are 1000000 rows in total.\n"
     ]
    }
   ],
   "source": [
    "train_purchases.show(5)\n",
    "print(f\"There are {train_purchases.count()} rows in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### item_features.csv\n",
    "\n",
    "This dataset helps us notice pattern in different objects. It links the item_id with their feature (such as the color of the item, the material, the type, etc ...).\n",
    "There are 3 columns :\n",
    "- item_id : the id of the item in the store.\n",
    "- feature_id : the id of the feature attached to the item.\n",
    "- value : the value of the feature for that item. (for example if the feature_category_id is the color, the feature_value_id could be a representation of \"red\", \"blue\", \"green\", etc ...)\n",
    "\n",
    "Here is a representation of a few of the rows in the item_features.csv dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+\n",
      "|item_id|feature_category_id|feature_value_id|\n",
      "+-------+-------------------+----------------+\n",
      "|      2|                 56|             365|\n",
      "|      2|                 62|             801|\n",
      "|      2|                 68|             351|\n",
      "|      2|                 33|             802|\n",
      "|      2|                 72|              75|\n",
      "+-------+-------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "There are 471751 rows in total.\n"
     ]
    }
   ],
   "source": [
    "item_features.show(5)\n",
    "print(f\"There are {item_features.count()} rows in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### candidate_items.csv\n",
    "\n",
    "This dataset contains all the item_id that are candidate to the recommandation system. It consists of a column of 4991 item_id.\n",
    "\n",
    "Here is a representation of a few of the rows in the candidate_items.csv dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|item_id|\n",
      "+-------+\n",
      "|      4|\n",
      "|      8|\n",
      "|      9|\n",
      "|     19|\n",
      "|     20|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "There are 4990 rows in total.\n"
     ]
    }
   ],
   "source": [
    "candidate_items.show(5)\n",
    "print(f\"There are {candidate_items.count()} rows in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 1 : Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will first check if there are missing values, NA, or Null values in the downloaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of missing values in candidate_items: 0\n",
      "Nb of missing values in item_features: 0\n",
      "Nb of missing values in train_purchases: 0\n",
      "Nb of missing values in train_sessions: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Nb of missing values in candidate_items:\", candidate_items.filter(candidate_items[\"item_id\"].isNull()).count())\n",
    "print(\"Nb of missing values in item_features:\", item_features.filter(item_features[\"item_id\"].isNull()).count())\n",
    "print(\"Nb of missing values in train_purchases:\", train_purchases.filter(train_purchases[\"item_id\"].isNull()).count())\n",
    "print(\"Nb of missing values in train_sessions:\", train_sessions.filter(train_sessions[\"item_id\"].isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we wondered if there are sessions for which the first item viewed was purchased (therefore it will not be in train_sessions but in train_purchases), and we also made sure that there are no sessions for which there were no purchases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of session id in train_sessions but not in train_purchases: 0\n",
      "Nb of session id in train_purchases but not in train_sessions: 0\n"
     ]
    }
   ],
   "source": [
    "t = [a[\"session_id\"] for a in train_sessions.select(\"session_id\").collect()]\n",
    "p = [a[\"session_id\"] for a in train_purchases.select(\"session_id\").collect()]\n",
    "\n",
    "print(f\"Nb of session id in train_sessions but not in train_purchases: {len(set(t).difference(set(p)))}\")\n",
    "print(f\"Nb of session id in train_purchases but not in train_sessions: {len(set(p).difference(set(t)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we wanted to know how much data we were working with so we counted the number of different item_id in each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of distinct item id in train_sessions: 23496\n",
      "Nb of distinct item id in train_purchases: 18907\n",
      "Nb of distinct item id in candidate_items: 4990\n",
      "Nb of distinct item id in item_features: 23691\n"
     ]
    }
   ],
   "source": [
    "a = train_sessions.select(\"item_id\").distinct().collect()\n",
    "b = train_purchases.select(\"item_id\").distinct().collect()\n",
    "c = candidate_items.select(\"item_id\").distinct().collect()\n",
    "d = item_features.select(\"item_id\").distinct().collect()\n",
    "print(f\"Nb of distinct item id in train_sessions: {len(a)}\")\n",
    "print(f\"Nb of distinct item id in train_purchases: {len(b)}\")\n",
    "print(f\"Nb of distinct item id in candidate_items: {len(c)}\")\n",
    "print(f\"Nb of distinct item id in item_features: {len(d)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot know if those items are the same or different, but by running the next cell we discover that item_features is complete. It contains all the items present in the other datasets. Therefore, we know that there exists 23691 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of item id in train_sessions but not in item_features: 0\n",
      "Nb of item id in train_purchases but not in item_features: 0\n",
      "Nb of item id in candidate_items but not in item_features: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nb of item id in train_sessions but not in item_features: {len(set(a).difference(set(d)))}\")\n",
    "print(f\"Nb of item id in train_purchases but not in item_features: {len(set(b).difference(set(d)))}\")\n",
    "print(f\"Nb of item id in candidate_items but not in item_features: {len(set(c).difference(set(d)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove sessions which end up with the purchase of a non candidate item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_purchases = candidate_items.join(train_purchases, \"item_id\", \"inner\")\n",
    "# ⚠️ Uncomment the following line to undersampling to reduce computing time\n",
    "# train_purchases = train_purchases.sample(withReplacement=False, fraction=0.01, seed=3)\n",
    "train_sessions = train_sessions.join(train_purchases.select(\"session_id\"), \"session_id\", \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform the dataset from a dataframe into an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps every session to its items and dates\n",
    "session_item_date_rdd = train_sessions.rdd.map(lambda x: (x[\"session_id\"], (x[\"item_id\"], x[\"date\"]))).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lists will contain the RDDs of the features along with their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "features_names = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date related features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an RDD that will be used to generate the following features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps every session to its dates\n",
    "session_date_rdd = session_item_date_rdd.mapValues(lambda x: (x[1])).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1) Month\n",
    "\n",
    "The month of the year (1 to 12) in which the session took place\n",
    "\n",
    "The reason for this feature is that the pieces of fashion are probably more likely to be bought depending on the month of the purchase. There is probably less swimsuits bought in December than in July, therefore we believe that if two items are bought in the same month, they could be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100800, 2), (270200, 9), (324400, 9), (504400, 5), (569200, 3)]\n"
     ]
    }
   ],
   "source": [
    "def get_month_feature():\n",
    "    return session_date_rdd.mapValues(lambda x: int(x.month))\\\n",
    "                            .reduceByKey(min)\n",
    "    \n",
    "month = get_month_feature()\n",
    "\n",
    "features.append(month)\n",
    "features_names.append(\"month\")\n",
    "\n",
    "print(month.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2) Season (Meteorological)\n",
    "\n",
    "The meteorological season of the year in which the session takes place.\n",
    "  - Winter : December, January, and February\n",
    "  - Spring : March, April, and May\n",
    "  - Summer : June, July, and Augustus\n",
    "  - Fall   : September, October, and November\n",
    "\n",
    "We add this feature even though it seems redudant with the months feature, because the month feature might be too restrictive (stuff bought in january or february can also be similar because of the season, especially in clothing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100800, 0), (270200, 3), (324400, 3), (504400, 1), (569200, 1)]\n"
     ]
    }
   ],
   "source": [
    "def get_season_feature():\n",
    "    return month.mapValues(get_season)\\\n",
    "                .reduceByKey(min)\n",
    "\n",
    "def get_season(month):\n",
    "    if month == 12 or month <= 2: return 0\n",
    "    elif 2 < month <= 5: return 1\n",
    "    elif 5 < month <= 8: return 2\n",
    "    elif 8 < month <= 11: return 3\n",
    "\n",
    "season = get_season_feature()\n",
    "\n",
    "features.append(season)\n",
    "features_names.append(\"season\")\n",
    "\n",
    "print(season.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3) Day of month\n",
    "\n",
    "We believe that the day of the month from 1 to 28, 29, 30 or 31 also has an influence on the purchases, because salaries are often paid close by the 1st of the month, therefore we might expect more purchase during that time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100800, 4), (270200, 1), (324400, 1), (504400, 1), (569200, 31)]\n"
     ]
    }
   ],
   "source": [
    "def get_day_of_month_feature():\n",
    "    return session_date_rdd.mapValues(lambda x: int(x.day))\\\n",
    "                            .reduceByKey(min)\n",
    "\n",
    "day_of_month = get_day_of_month_feature()\n",
    "\n",
    "features.append(day_of_month)\n",
    "features_names.append(\"day_of_month\")\n",
    "\n",
    "print(day_of_month.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4) Weekday\n",
    "\n",
    "This feature indicates what day of the week (0 to 6, starting on monday) the session happened as we believe this may have an influence, because there is more time to browse on the weekend than on week days where people are probably working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100800, 4), (270200, 2), (324400, 2), (504400, 6), (569200, 3)]\n"
     ]
    }
   ],
   "source": [
    "def get_weekday_feature():\n",
    "    return session_date_rdd.mapValues(lambda x: int(x.strftime(\"%w\")))\\\n",
    "                            .reduceByKey(min)\n",
    "\n",
    "weekday = get_weekday_feature()\n",
    "\n",
    "features.append(weekday)\n",
    "features_names.append(\"weekday\")\n",
    "\n",
    "print(weekday.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Weekend\n",
    "\n",
    "A binary feature to tells us if the session happened on a weekend. We are not sure if the weekday feature is too specific, this should allow the model to differentiate the situation better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100800, 0), (270200, 0), (324400, 0), (504400, 1), (569200, 0)]\n"
     ]
    }
   ],
   "source": [
    "def get_weekend_feature():\n",
    "    return weekday.mapValues(lambda x: int(x in (5, 6)))\n",
    "\n",
    "weekend = get_weekend_feature()\n",
    "\n",
    "features.append(weekend)\n",
    "features_names.append(\"weekend\")\n",
    "\n",
    "print(weekend.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 6) Hour\n",
    "\n",
    "This feature give us the 24-hour format hour of the session. The justification is that there are probably specific items bought late in the night, or other items bought a few hours after the end of workday.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100800, 18), (270200, 18), (324400, 21), (504400, 15), (569200, 12)]\n"
     ]
    }
   ],
   "source": [
    "def get_hour_feature():\n",
    "    return session_date_rdd.mapValues(lambda x: int(x.strftime(\"%H\")))\\\n",
    "                            .reduceByKey(min)\n",
    "\n",
    "hour = get_hour_feature()\n",
    "\n",
    "features.append(hour)\n",
    "features_names.append(\"hour\")\n",
    "\n",
    "print(hour.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) Day period\n",
    "To avoid the features of being too selective, we devided the day into 4 periods, the morning, the afternoon, the evening, and the night. We believe that items bought in the same period will share similarities, for example when seeing an advertisment when driving from/to work, or when you are browsing the internet late at night.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100800, 3), (270200, 3), (324400, 2), (504400, 1), (569200, 3)]\n"
     ]
    }
   ],
   "source": [
    "def get_day_period_feature():\n",
    "    return hour.mapValues(get_day)\n",
    "\n",
    "def get_day(x):\n",
    "    if 6 < x < 12: return 0\n",
    "    elif 12 < x < 18: return 1\n",
    "    elif 18 < x < 22: return 2\n",
    "    else: return 3\n",
    "    \n",
    "day_period = get_day_period_feature()\n",
    "\n",
    "features.append(day_period)\n",
    "features_names.append(\"day_period\")\n",
    "\n",
    "print(day_period.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8) Night\n",
    "A boolean variable to indicate if the session was during the night, or the day to further divide the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100800, 1), (270200, 1), (324400, 0), (504400, 0), (569200, 1)]\n"
     ]
    }
   ],
   "source": [
    "def get_night_feature():\n",
    "    return day_period.mapValues(lambda x: int(x == 3))\n",
    "\n",
    "night = get_night_feature()\n",
    "\n",
    "features.append(night)\n",
    "features_names.append(\"night\")\n",
    "\n",
    "print(night.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9) Duration of the session\n",
    "\n",
    "Short sessions might be straight to the point from seeing an object to buying another one, maybe it can tell us something about the item feature that was so great it was bought so fast/slow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100800, 0.0), (270200, 325.706), (324400, 61.3), (504400, 0.0), (569200, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "def get_duration_feature():\n",
    "    return session_date_rdd.groupByKey()\\\n",
    "                            .mapValues(get_session_duration)\n",
    "\n",
    "def get_session_duration(dates):\n",
    "    dates = list(dates)\n",
    "    dates.sort()\n",
    "    return (dates[-1] - dates[0]).total_seconds() if len(dates) >= 2 else 0.0\n",
    "\n",
    "duration = get_duration_feature()\n",
    "\n",
    "features.append(duration)\n",
    "features_names.append(\"duration\")\n",
    "\n",
    "print(duration.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 10) Average time between consecutive item views\n",
    "\n",
    "We believe that buyers who saw a lot of items in a very short amount of times might belong to the same category of consumers (for example frequent customer, or IT-litterate people) in a way that they might share similar interests in fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100800, 0), (270200, 27), (324400, 61), (504400, 0), (569200, 0)]\n"
     ]
    }
   ],
   "source": [
    "def get_average_time_feature():\n",
    "    return session_date_rdd.groupByKey()\\\n",
    "                            .mapValues(get_average_time)\n",
    "\n",
    "def get_average_time(dates):\n",
    "    import datetime\n",
    "    dates = sorted(list(dates))\n",
    "    avgs = [dates[i+1] - dates[i] for i in range(len(dates)-1)]\n",
    "    return round((sum(avgs, datetime.timedelta())/len(avgs)).total_seconds()) if len(avgs) > 0 else 0\n",
    "\n",
    "average_time = get_average_time_feature()\n",
    "\n",
    "features.append(average_time)\n",
    "features_names.append(\"average_time\")\n",
    "\n",
    "print(average_time.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session related features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an RDD that will be used to generate the following features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps every session to its items\n",
    "session_item_rdd = session_item_date_rdd.mapValues(lambda x: (x[0])).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11) Number of items\n",
    "\n",
    "It tells us the number of items seen in the session. We believe that alone it is useless but by combining it with the following features, it could become useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100800, 1), (270200, 13), (324400, 2), (504400, 1), (569200, 1)]\n"
     ]
    }
   ],
   "source": [
    "def get_length_feature():\n",
    "    return session_item_rdd.groupByKey().mapValues(len)\n",
    "\n",
    "length = get_length_feature()\n",
    "\n",
    "features.append(length)\n",
    "features_names.append(\"length\")\n",
    "\n",
    "print(length.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 12) Number of distinct items\n",
    "\n",
    "It tells us the number of distinct items viewed in the session. We believe that this feature could add a nuance to the feature \"number of items\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100800, 1), (270200, 12), (324400, 2), (504400, 1), (569200, 1)]\n"
     ]
    }
   ],
   "source": [
    "def get_distinct_nb_feature():\n",
    "    return session_item_rdd.groupByKey().mapValues(lambda x: len(set(x)))\n",
    "\n",
    "distinct_nb = get_distinct_nb_feature()\n",
    "\n",
    "features.append(distinct_nb)\n",
    "features_names.append(\"distinct_nb\")\n",
    "\n",
    "print(distinct_nb.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Items related features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13) Most viewed item\n",
    "\n",
    "It tells us the item that was seen the most amount of times. We believe that this item has a high probability of being the item bought at the end of the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100800, 18407), (270200, 39), (324400, 7267), (504400, 7717), (569200, 8702)]\n"
     ]
    }
   ],
   "source": [
    "def get_most_viewed_item_feature():\n",
    "    return session_item_rdd.groupByKey().mapValues(get_most_viewed_item)\n",
    "\n",
    "def get_most_viewed_item(items):\n",
    "    items = list(items)\n",
    "    items.sort()\n",
    "    most_viewed = (None, -1)\n",
    "    \n",
    "    last_viewed = items[0]\n",
    "    cnt = 0\n",
    "    \n",
    "    for item in items:\n",
    "        if last_viewed != item:\n",
    "            if cnt > most_viewed[1]:\n",
    "                most_viewed = (last_viewed, cnt)\n",
    "                cnt = 1\n",
    "                last_viewed = item\n",
    "        else:\n",
    "            cnt += 1\n",
    "            \n",
    "    if cnt > most_viewed[1]:\n",
    "        most_viewed = (last_viewed, cnt)\n",
    "    \n",
    "    return most_viewed[0]\n",
    "    \n",
    "most_viewed_item = get_most_viewed_item_feature()\n",
    "\n",
    "features.append(most_viewed_item)\n",
    "features_names.append(\"most_viewed_item\")\n",
    "\n",
    "print(most_viewed_item.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14) Longest viewed item\n",
    "\n",
    "If a customer spends a lot of time viewing an item, it might be because he is interested in its caracteristics, and therefore more likely to buy that item or a similar one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100800, 18407), (270200, 8700), (324400, 7267), (504400, 7717), (569200, 8702)]\n"
     ]
    }
   ],
   "source": [
    "def get_longest_item_feature():\n",
    "    return train_sessions.rdd.map(lambda x: (x[\"session_id\"], (x[\"item_id\"], x[\"date\"]))).\\\n",
    "                    groupByKey().\\\n",
    "                    mapValues(get_longest_item)\n",
    "\n",
    "def get_longest_item(items):\n",
    "    items = list(items)\n",
    "    t = [items[i+1][1] - items[i][1] for i in range(len(items)-1)]\n",
    "    return items[np.argmax(t)][0] if len(t) > 0 else items[0][0]\n",
    "\n",
    "longest_item = get_longest_item_feature()\n",
    "\n",
    "features.append(longest_item)\n",
    "features_names.append(\"longest_item\")\n",
    "\n",
    "print(longest_item.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 15) Last item\n",
    "\n",
    "It tells us the last item that was seen in the session. The last item seen in the session should be closer to the purchased item than the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100800, 18407), (270200, 19819), (324400, 7267), (504400, 7717), (569200, 8702)]\n"
     ]
    }
   ],
   "source": [
    "def get_last_item_feature():\n",
    "    return session_item_date_rdd.groupByKey()\\\n",
    "                                .mapValues(lambda x: max(x, key=lambda i: i[1])[0])\n",
    "\n",
    "def get_last_item(items):\n",
    "    return max(list(items), key=lambda i: i[1])[0]\n",
    "\n",
    "last_item = get_last_item_feature()\n",
    "\n",
    "features.append(last_item)\n",
    "features_names.append(\"last_item\")\n",
    "\n",
    "print(last_item.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16) Is the most viewed item also the longest viewed item ?\n",
    "\n",
    "We checked if the item that was most often seen is also the item that was viewed for the longest time. We believe that if those two variables point to the same item, it is very likely that the item bought is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100800, 1), (270200, 0), (324400, 1), (504400, 1), (569200, 1)]\n"
     ]
    }
   ],
   "source": [
    "def get_most_longest_equality_feature():\n",
    "    return most_viewed_item.join(longest_item)\\\n",
    "                            .mapValues(lambda x: int(x[0] == x[1]))\n",
    "\n",
    "most_longest_equality = get_most_longest_equality_feature()\n",
    "\n",
    "features.append(most_longest_equality)\n",
    "features_names.append(\"most_longest_equality\")\n",
    "\n",
    "print(most_longest_equality.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17) Is the most viewed item also the last item ?\n",
    "\n",
    "We checked if the item that was most often seen is also the last item viewed. We believe that if those two variables point to the same item, it is very likely that the item bought is similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100800, 1), (270200, 0), (324400, 1), (504400, 1), (569200, 1)]\n"
     ]
    }
   ],
   "source": [
    "def get_most_last_equality_feature():\n",
    "    return most_viewed_item.join(last_item)\\\n",
    "                            .mapValues(lambda x: int(x[0] == x[1]))\n",
    "\n",
    "most_last_equality = get_most_last_equality_feature()\n",
    "\n",
    "features.append(most_last_equality)\n",
    "features_names.append(\"most_last_equality\")\n",
    "\n",
    "print(most_last_equality.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18) Is the longest viewed item also the last item ?\n",
    "\n",
    "We checked if the item that was was viewed for the longest time is also the last item seen. We believe that if those two variables point to the same item, it is very likely that the item bought is similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100800, 1), (270200, 0), (324400, 1), (504400, 1), (569200, 1)]\n"
     ]
    }
   ],
   "source": [
    "def get_longest_last_equality_feature():\n",
    "    return longest_item.join(last_item)\\\n",
    "                    .mapValues(lambda x: int(x[0] == x[1]))\n",
    "\n",
    "longest_last_equality = get_longest_last_equality_feature()\n",
    "\n",
    "features.append(longest_last_equality)\n",
    "features_names.append(\"longest_last_equality\")\n",
    "\n",
    "print(longest_last_equality.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Items' categories related features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create two RDDs that will be used to generate the following features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps every item to its features\n",
    "item_features_rdd = item_features.rdd.map(lambda x: (x[\"item_id\"], (x[\"feature_category_id\"], x[\"feature_value_id\"])))\\\n",
    "                                .groupByKey()\\\n",
    "                                .mapValues(lambda x: [(a,b) for a, b in x])\n",
    "\n",
    "# Maps every session to the features of its items\n",
    "session_item_features_rdd = item_features_rdd.join(train_sessions.rdd.map(lambda x: (x[\"item_id\"], x[\"session_id\"])))\\\n",
    "                                            .map(lambda x: (x[1][1], x[1][0]))\\\n",
    "                                            .groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19) Most common number of categories of all items seen\n",
    "\n",
    "This feature tells us for each session the number of categories that appears the most. We believe that if two items have the same number of categories the are probably similar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1799214, 20), (295526, 24), (1798608, 18), (1255632, 18), (1212808, 25)]\n"
     ]
    }
   ],
   "source": [
    "def get_categories_nb_feature():\n",
    "    return session_item_features_rdd.mapValues(get_categories_nb)\n",
    "\n",
    "def get_categories_nb(cat):\n",
    "    lst = [len(c) for c in cat]\n",
    "    return max(set(lst), key=lst.count)\n",
    "    \n",
    "categories_nb = get_categories_nb_feature()\n",
    "\n",
    "features.append(categories_nb)\n",
    "features_names.append(\"categories_nb\")\n",
    "\n",
    "print(categories_nb.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20 - 21) Most common category with its count\n",
    "We regroup in one column the most common category id and its count so that similar categories and similar counts are closer in the search space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate for each session the most present category and the number of times it appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_present_category_with_count(categories_i):\n",
    "    categories = [cat for cat_i in categories_i for cat in cat_i]    \n",
    "    categories.sort()\n",
    "    \n",
    "    most_viewed = (None, -1)\n",
    "    last_viewed = categories[0]\n",
    "    cnt = 0\n",
    "    \n",
    "    for category in categories:\n",
    "        if last_viewed != category:\n",
    "            if cnt > most_viewed[1]:\n",
    "                most_viewed = (last_viewed, cnt)\n",
    "                cnt = 1\n",
    "                last_viewed = category\n",
    "        else:\n",
    "            cnt += 1\n",
    "            \n",
    "    if cnt > most_viewed[1]:\n",
    "        most_viewed = (last_viewed, cnt)\n",
    "    \n",
    "    return most_viewed[0], most_viewed[1]\n",
    "\n",
    "most_present_category_with_count = session_item_features_rdd.mapValues(get_most_present_category_with_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the RDD above in two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models in pyspark require the features to have a numeric value. So to solve easily this issue we created a simple function that return the sum of ($category$ * 1000) and the $value$. We chose 1000 as the maximum value that $value$ can take is less than 1000, this ensures no collision.\n",
    "\n",
    "For example: (category, value) : (66, 109) -> 66 000 + 109 = 66 109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1799214, 4618), (295526, 3793), (1798608, 3793), (1255632, 4618), (1212808, 3793)]\n"
     ]
    }
   ],
   "source": [
    "def get_most_present_category_feature():\n",
    "    return most_present_category_with_count.mapValues(lambda x: x[0][0]*1000 + x[0][1])\n",
    "\n",
    "most_present_category = get_most_present_category_feature()\n",
    "\n",
    "features.append(most_present_category)\n",
    "features_names.append(\"most_present_category\")\n",
    "\n",
    "print(most_present_category.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1799214, 3), (295526, 38), (1798608, 6), (1255632, 5), (1212808, 12)]\n"
     ]
    }
   ],
   "source": [
    "def get_most_present_category_count_feature():\n",
    "    return most_present_category_with_count.mapValues(lambda x: x[1])\n",
    "\n",
    "most_present_category_count = get_most_present_category_count_feature()\n",
    "\n",
    "features.append(most_present_category_count)\n",
    "features_names.append(\"most_present_category_count\")\n",
    "\n",
    "print(most_present_category_count.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 22) Number of categories of the most viewed item\n",
    "\n",
    "This variable computes the number of categories of the most viewed item. It might give us information about the bought product since it might have a similar number of categories, if the items are similar. For example if for a jacket we have 4 categories (size color cut and material), and the item bought also has 4 categories, it is possible that it is also a jacket, since gloves for example could have only 2 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3201600, 18), (2902600, 18), (24801, 18), (1705601, 18), (2194201, 18)]\n"
     ]
    }
   ],
   "source": [
    "def get_nb_categories_most_viewed_item_feature():\n",
    "    return most_viewed_item.map(lambda x: (x[1], x[0]))\\\n",
    "                            .join(item_features_rdd)\\\n",
    "                            .map(lambda x: (x[1][0], len(x[1][1])))\n",
    "\n",
    "nb_categories_most_viewed_item = get_nb_categories_most_viewed_item_feature()\n",
    "\n",
    "features.append(nb_categories_most_viewed_item)\n",
    "features_names.append(\"nb_categories_most_viewed_item\")\n",
    "\n",
    "print(nb_categories_most_viewed_item.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 23) Number of categories of the longest viewed item\n",
    "\n",
    "This variable computes the number of categories of the most longest viewed item. Like the previous feature, it might give us information about the bought product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3201600, 18), (1062200, 18), (2123002, 18), (1904603, 18), (2862406, 18)]\n"
     ]
    }
   ],
   "source": [
    "def get_nb_categories_longest_item_feature():\n",
    "    return longest_item.map(lambda x: (x[1], x[0]))\\\n",
    "                        .join(item_features_rdd)\\\n",
    "                        .map(lambda x: (x[1][0], len(x[1][1])))\n",
    "\n",
    "nb_categories_longest_item = get_nb_categories_longest_item_feature()\n",
    "\n",
    "features.append(nb_categories_longest_item)\n",
    "features_names.append(\"nb_categories_longest_item\")\n",
    "\n",
    "print(nb_categories_longest_item.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 24) Number of categories of the last item.\n",
    "\n",
    "This variable computes the number of categories of the last item. Like the previous feature, it might give us information about the bought product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2357800, 25), (398614, 25), (598820, 25), (2366227, 25), (2564630, 25)]\n"
     ]
    }
   ],
   "source": [
    "def get_nb_categories_last_item_feature():\n",
    "    return last_item.map(lambda x: (x[1], x[0]))\\\n",
    "                    .join(item_features_rdd)\\\n",
    "                    .map(lambda x: (x[1][0], len(x[1][1])))\n",
    "\n",
    "nb_categories_last_item = get_nb_categories_last_item_feature()\n",
    "\n",
    "features.append(nb_categories_last_item)\n",
    "features_names.append(\"nb_categories_last_item\")\n",
    "\n",
    "print(nb_categories_last_item.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "All of these engineered features will be put in the same RDD with the id of the session as the key and the different features as the value.\n",
    "\n",
    "First we add to the values of the features RDDs the index of the feature\n",
    "\n",
    "Then we take the union of all the features RDDs.\n",
    "\n",
    "Finally we group the elements of the union by their key, the session_id, and we create a list containing the label as well as all the features according to their index.\n",
    "\n",
    "We end up with an RDD which has this structure.\n",
    "\n",
    "session_id : [label, feature_1, feature_2, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "purchase_item_rdd = train_purchases.rdd.map(lambda x: (x[\"session_id\"], x[\"item_id\"])).cache()\n",
    "\n",
    "def group_full_rdd(x):\n",
    "    temp = [None] * (len(features_names)+1)\n",
    "    for item in x:\n",
    "        temp[item[0]] = item[1]\n",
    "    return temp\n",
    "        \n",
    "def get_full_rdd():\n",
    "    temp = purchase_item_rdd.mapValues(lambda x: (0, x))\n",
    "    for i, feature in enumerate(features):\n",
    "        temp = temp.union(feature.mapValues(lambda x: (i+1, x)))\n",
    "    \n",
    "    temp = temp.groupByKey().mapValues(group_full_rdd)\n",
    "    return temp\n",
    "\n",
    "BIG_RDD = get_full_rdd().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(129978, [13269, 4, 1, 28, 3, 0, 23, 3, 1, 149.714, 30, 6, 4, 1373, 20271, 1373, 0, 1, 0, 25, 3793, 6, 25, 25, 25])]\n"
     ]
    }
   ],
   "source": [
    "print(BIG_RDD.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 2 : Feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Ranking algorithm\n",
    "\n",
    "According to G. Bontempi's handbook \"Statistical foundations of machine learning\", the ranking methods follow these steps:\n",
    "    \n",
    "    1) Calculate for each feature its relevance with the output variable using a univariate measure.\n",
    "    \n",
    "    2) Sort their relevance in descending order.\n",
    "    \n",
    "    3) Select the top k features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have a lot of categorical features as well as a categorical output, we chose to use mutual information as a univariate measure.\n",
    "\n",
    "$$ I(X;Y) = H(Y) - H(Y|X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Entropy-mutual-information-relative-entropy-relation-diagram.svg/1200px-Entropy-mutual-information-relative-entropy-relation-diagram.svg.png\" width=\"400\">\n",
    "Taken from https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Entropy-mutual-information-relative-entropy-relation-diagram.svg/1200px-Entropy-mutual-information-relative-entropy-relation-diagram.svg.png "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's compute $H(Y)$ in map reduce.\n",
    "\n",
    "$$\\displaystyle \\mathrm {H} (Y)=-\\sum _{y\\in {\\mathcal {Y}}}p(y)\\log _{2}p(y)$$\n",
    "\n",
    "\n",
    "\n",
    "Based on https://en.wikipedia.org/wiki/Entropy_(information_theory)\n",
    "\n",
    "First for each session we produce one element with its label as the key and $1$ as the value.\n",
    "\n",
    "Then reduce by key by adding the values, this gets us the count for each label.\n",
    "\n",
    "After that we map to get $p(y)$ by dividing the count by the total number of elements.\n",
    "\n",
    "Finally we map each $p(y)$ to $-p(y)\\log _{2}p(x)$, which represents all the terms of the equation above, and we reduce them by adding them.\n",
    "\n",
    "[^1]: lol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = purchase_item_rdd.count()\n",
    "\n",
    "total_count_broadcast = spark.sparkContext.broadcast(total_count)\n",
    "\n",
    "H_y = BIG_RDD.map(lambda x: (x[1][0], 1))\\\n",
    "            .reduceByKey(np.add)\\\n",
    "            .map(lambda x: x[1]/total_count_broadcast.value)\\\n",
    "            .map(lambda x: -x * np.log2(x))\\\n",
    "            .reduce(np.add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the value of $H(Y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.03202146098545"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate $H(Y|X)$ of each feature in map reduce.\n",
    "\n",
    "$$\\displaystyle \\mathrm {H} (Y|X)=-\\sum _{y,x\\in {\\mathcal {Y}}\\times {\\mathcal {X}}}p_{Y,X}(y,x)\\log {\\frac {p_{Y,X}(y,x)}{p_{X}(x)}}$$\n",
    "Based on https://en.wikipedia.org/wiki/Entropy_(information_theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This RDD will contain the number of elements which correspond to the same feature_index and have the same value and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (feature_index, feature_value, label) : count\n",
    "count_yx = BIG_RDD.flatMap(lambda x: [((i, x[1][i], x[1][0]), 1) for i in range(1, len(x[1]))])\\\n",
    "                    .reduceByKey(np.add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This RDD will contain the $p$ of the elements which correspond to the same feature_index and have the same value and label.\n",
    "\n",
    "In other words, this will contain the $p_{Y,X}(y,x)$ for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (feature_index, feature_value, label) : p\n",
    "p_yx = count_yx.mapValues(lambda x: x / total_count_broadcast.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This RDD will contain the number of elements which correspond to the same feature_index and have the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (feature_index, feature_value) : count\n",
    "count_x = count_yx.map(lambda x: ((x[0][0], x[0][1]), x[1]))\\\n",
    "                    .reduceByKey(np.add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This RDD will contain the $p$ of the elements which correspond to the same feature_index and have the same value.\n",
    "\n",
    "In other words, this will contain the $p_{X}(x)$ for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (feature_index, feature_value) : p\n",
    "p_x = count_x.mapValues(lambda x: x / total_count_broadcast.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate $H(y|x)$ for each feature.\n",
    "\n",
    "First we take the RDD which contains the $p_{Y,X}(y,x)$ for each feature and we join it with the RDD which contains the $p_{X}(x)$ for each feature.\n",
    "\n",
    "Finally we map each element to $p_{Y,X}(y,x)\\log {\\frac {p_{Y,X}(y,x)}{p_{X}(x)}}$, which represents all the terms of the equation above, and we reduce them by adding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h_yx_term(x):\n",
    "    p_yx = x[0]\n",
    "    p_y = x[1]\n",
    "    h_yx_term = - p_yx * np.log2(p_yx/p_y)\n",
    "    \n",
    "    return h_yx_term\n",
    "\n",
    "H_yx = p_yx.map(lambda x: ((x[0][0], x[0][1]), x[1]))\\\n",
    "            .join(p_x)\\\n",
    "            .map(lambda x: (x[0][0], get_h_yx_term(x[1])))\\\n",
    "            .reduceByKey(np.add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the value of $H(Y|X)$ for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 10.075723915996821),\n",
       " (2, 10.393079295379358),\n",
       " (3, 10.662043448070813),\n",
       " (4, 10.958292053275848),\n",
       " (5, 11.018113765358375),\n",
       " (6, 10.837239056117252),\n",
       " (7, 10.999397527159932),\n",
       " (8, 11.02249534813991),\n",
       " (9, 3.57164547680823),\n",
       " (10, 9.007892033810464),\n",
       " (11, 10.785789841027354),\n",
       " (12, 10.811317578751712),\n",
       " (13, 5.260058196644147),\n",
       " (14, 5.170970290288833),\n",
       " (15, 4.694100578387632),\n",
       " (16, 11.006942193247763),\n",
       " (17, 11.005879277544992),\n",
       " (18, 11.00046796553993),\n",
       " (19, 10.336253059083456),\n",
       " (20, 10.451879782370915),\n",
       " (21, 10.830148946392256),\n",
       " (22, 10.364923349486572),\n",
       " (23, 10.423365463383101),\n",
       " (24, 10.199048539517932)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_yx.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we can compute $I(X;Y)$ for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_y_broadcast = spark.sparkContext.broadcast(H_y)\n",
    "\n",
    "features_mi = H_yx.mapValues(lambda x: H_y_broadcast.value - x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's sort and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features ranking:\n",
      "\t- duration (7.460375984177219)\n",
      "\t- last_item (6.3379208825978175)\n",
      "\t- longest_item (5.861051170696617)\n",
      "\t- most_viewed_item (5.771963264341302)\n",
      "\t- average_time (2.0241294271749855)\n",
      "\t- month (0.9562975449886277)\n",
      "\t- nb_categories_last_item (0.8329729214675172)\n",
      "\t- categories_nb (0.6957684019019936)\n",
      "\t- nb_categories_most_viewed_item (0.667098111498877)\n",
      "\t- season (0.6389421656060907)\n",
      "\t- nb_categories_longest_item (0.608655997602348)\n",
      "\t- most_present_category (0.5801416786145346)\n",
      "\t- day_of_month (0.36997801291463617)\n",
      "\t- length (0.2462316199580954)\n",
      "\t- distinct_nb (0.22070388223373705)\n",
      "\t- most_present_category_count (0.20187251459319278)\n",
      "\t- hour (0.19478240486819764)\n",
      "\t- weekday (0.07372940770960135)\n",
      "\t- day_period (0.032623933825517426)\n",
      "\t- longest_last_equality (0.031553495445519886)\n",
      "\t- most_last_equality (0.0261421834404576)\n",
      "\t- most_longest_equality (0.025079267737686095)\n",
      "\t- weekend (0.013907695627073835)\n",
      "\t- night (0.009526112845538393)\n"
     ]
    }
   ],
   "source": [
    "features_mi.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Features ranking:\")\n",
    "\n",
    "for i in features_mi:\n",
    "    print(f\"\\t- {features_names[i[0]-1]} ({i[1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Forward feature selection algorithm\n",
    "\n",
    "According to G. Bontempi's handbook \"Statistical foundations of machine learning\", the forward feature selection algorithms are part of wrapping search strategies and work as follows:\n",
    "    \n",
    "    First, we start with no selected features.\n",
    "    \n",
    "    Then, we select the feature which returns the lowest generalisation error.\n",
    "    \n",
    "    After that, we select the feature which, with the previously selected features, returns the lowest generalisation error.\n",
    "    \n",
    "    The last step is repeated until there is no improvement or the maximum number of features is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the pseudo code.\n",
    "```python\n",
    "def forward_feature_selection(ALL_FEATURES):\n",
    "    remaining_features <- ALL_FEATURES\n",
    "    selected_features <- []\n",
    "    do \n",
    "        candidate_feature <- find_lowest_generalisation_error(remaining_features)\n",
    "\n",
    "        if improvement:\n",
    "            selected_features.add(candidate_feature)\n",
    "            remaining_features.remove(candidate_feature)\n",
    "\n",
    "    while len(remaining_features) > 0 or no_improvement\n",
    "\n",
    "    return remaining_features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the RDD to train the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "all_features_rdd = BIG_RDD.sample(withReplacement=False, fraction=0.01, seed=3).map(lambda x: (x[1][0], x[1][1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can launch the forward feature selection algorithm.\n",
    "\n",
    "# ⚠️\n",
    "\n",
    "In order to calculate the generalisation error, the use of cross validation would have been better. But due to the large computational power needed to run the cross validation, a single split of the data was performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = candidate_items.rdd.map(lambda x: x[\"item_id\"]).reduce(max)+1\n",
    "selected_features_indices = []\n",
    "remaining_features_indices = list(range(len(features)))\n",
    "improvement = True\n",
    "last_accuracy = -1\n",
    "\n",
    "while (len(selected_features_indices) < len(features)) and (improvement):\n",
    "    current_accuracy = []\n",
    "    selected_features_indices_broadcast = spark.sparkContext.broadcast(selected_features_indices)\n",
    "    print(f\"Searching feature {len(selected_features_indices)+1}: \", end=\"\")\n",
    "    for current_feature_index in remaining_features_indices:\n",
    "        print(\"|\", end=\"\")\n",
    "        current_feature_index_broadcast = spark.sparkContext.broadcast(current_feature_index)\n",
    "        selected_features_rdd = all_features_rdd.map(lambda x: LabeledPoint(x[0], [x[1][i] for i in selected_features_indices_broadcast.value] + [x[1][current_feature_index_broadcast.value]]))\n",
    "        \n",
    "        training, test = selected_features_rdd.randomSplit([0.6, 0.4], seed = 515)\n",
    "        \n",
    "        model = DecisionTree.trainClassifier(training, nb_classes, {}, maxDepth=1, maxBins=2)\n",
    "        \n",
    "        predictions = model.predict(test.map(lambda x: x.features))\n",
    "        labelsAndPredictions = test.map(lambda x: x.label).zip(predictions)\n",
    "        \n",
    "        accuracy = 1.0 * labelsAndPredictions.filter(lambda x: x[0] == x[1]).count() / test.count()\n",
    "\n",
    "        current_accuracy.append((accuracy, current_feature_index))\n",
    "    print()\n",
    "    best_feature = max(current_accuracy, key=lambda x: x[0])\n",
    "    \n",
    "    if best_feature[0] > last_accuracy:\n",
    "        last_accuracy = best_feature[0]\n",
    "        best_feature_index = best_feature[1]\n",
    "        selected_features_indices.append(best_feature_index)\n",
    "        remaining_features_indices.remove(best_feature_index)\n",
    "\n",
    "    else:\n",
    "        improvement = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print the features selected by our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_feature_names = [features_name[i] for i in selected_feature_indices]\n",
    "print(f\"The best features are: \", end=\"\")\n",
    "print(\", \".join(selected_feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚠️\n",
    "Due to the imposibility for the cluster to run the algorithm above in a reasonable amount of time, we just select the 12 best features according to the ranking algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_indices = [features_mi[i][0]-1 for i in range(12)]\n",
    "selected_features_names = [features_names[i] for i in selected_feature_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 3 : Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The model should be trained on the data with the selected features and should returned the predictions required by the competition. For each test session, the model should return 100 candidates items with the highest chance of being purchased. This restricts the differents models that can be used. Therefore we selected the Decision Tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first transform our rdd which contains all the data into a dataframe which the classifier can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "\n",
    "def collector(x):\n",
    "    dico = {}\n",
    "    dico[\"session_id\"] = x[0]\n",
    "    dico[\"item_id\"] = x[1][0]\n",
    "    \n",
    "    for i in range(len(features_names)):\n",
    "        dico[features_names[i]] = x[1][i+1]\n",
    "        \n",
    "    return dico\n",
    "\n",
    "df = BIG_RDD.map(lambda x: Row(**collector(x))).toDF()\n",
    "df = df.fillna(0)\n",
    "si = StringIndexer(inputCol=\"item_id\", outputCol=\"label\").fit(candidate_items)\n",
    "indexed_items_df = si.transform(candidate_items)\n",
    "df = si.transform(df)\n",
    "df = VectorAssembler(inputCols = selected_features_names, outputCol = \"features\").transform(df)\n",
    "df = df.select([\"features\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionTree = DecisionTreeClassifier(maxDepth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can test the accuracy of our model with cross validation. We chose $K$ = 4 to split the dataset into 75 % training and 25 % testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "grid = ParamGridBuilder().addGrid(decisionTree.featuresCol, [\"features\"]).build()\n",
    "cv = CrossValidator(estimator=decisionTree, estimatorParamMaps=grid, numFolds=4, evaluator=evaluator, parallelism=4)\n",
    "cvModel = cv.fit(df)\n",
    "\n",
    "print(\"Average accuracy:\", cvModel.avgMetrics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy of our model can seem quite low, but this can be explained by the large number of different items in the dataset. It is approxamively 100 % better than just selecting items at random. Indeed, an algorithm based only at random would have an accuracy of 2e-4. Even in the RecSys Challenge 2022, it is asked for each session to give 100 items which have the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the predictions for the RecSys Challenge 2022's leaderboard.\n",
    "\n",
    "First we load the specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_sessions = spark.read.csv(\"dressipi_recsys2022/test_leaderboard_sessions.csv\", header=True)\n",
    "leaderboard_sessions = leaderboard_sessions.withColumn(\"session_id\", leaderboard_sessions[\"session_id\"].cast(\"int\"))\n",
    "leaderboard_sessions = leaderboard_sessions.withColumn(\"item_id\", leaderboard_sessions[\"item_id\"].cast(\"int\"))\n",
    "leaderboard_sessions = leaderboard_sessions.withColumn(\"date\", leaderboard_sessions[\"date\"].cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compute the values of all the different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_item_date_rdd = leaderboard_sessions.rdd.map(lambda x: (x[\"session_id\"], (x[\"item_id\"], x[\"date\"]))).cache()\n",
    "session_item_rdd = session_item_date_rdd.mapValues(lambda x: (x[0])).cache()\n",
    "session_date_rdd = session_item_date_rdd.mapValues(lambda x: (x[1])).cache()\n",
    "\n",
    "session_item_features_rdd = item_features_rdd.join(leaderboard_sessions.rdd.map(lambda x: (x[\"item_id\"], x[\"session_id\"])))\\\n",
    "                                            .map(lambda x: (x[1][1], x[1][0]))\\\n",
    "                                            .groupByKey()\n",
    "\n",
    "item_features_rdd = item_features.rdd.map(lambda x: (x[\"item_id\"], (x[\"feature_category_id\"], x[\"feature_value_id\"])))\\\n",
    "                                .groupByKey()\\\n",
    "                                .mapValues(lambda x: [(a,b) for a, b in x])\n",
    "\n",
    "session_item_features_rdd = item_features_rdd.join(leaderboard_sessions.rdd.map(lambda x: (x[\"item_id\"], x[\"session_id\"])))\\\n",
    "                                            .map(lambda x: (x[1][1], x[1][0]))\\\n",
    "                                            .groupByKey()\n",
    "\n",
    "\n",
    "month = get_month_feature()\n",
    "season = get_season_feature()\n",
    "day_of_month = get_day_of_month_feature()\n",
    "weekday = get_weekday_feature()\n",
    "weekend = get_weekend_feature()\n",
    "hour = get_hour_feature()\n",
    "day_period = get_day_period_feature()\n",
    "night = get_night_feature()\n",
    "duration = get_duration_feature()\n",
    "average_time = get_average_time_feature()\n",
    "\n",
    "length = get_length_feature()\n",
    "distinct_nb = get_distinct_nb_feature()\n",
    "\n",
    "most_viewed_item = get_most_viewed_item_feature()\n",
    "longest_item = get_longest_item_feature()\n",
    "last_item = get_last_item_feature()\n",
    "most_longest_equality = get_most_longest_equality_feature()\n",
    "most_last_equality = get_most_last_equality_feature()\n",
    "longest_last_equality = get_longest_last_equality_feature()\n",
    "\n",
    "categories_nb = get_categories_nb_feature()\n",
    "\n",
    "most_present_category_with_count = session_item_features_rdd.mapValues(get_most_present_category_with_count)\n",
    "\n",
    "most_present_category = get_most_present_category_feature()\n",
    "most_present_category_count = get_most_present_category_count_feature()\n",
    "nb_categories_most_viewed_item = get_nb_categories_most_viewed_item_feature()\n",
    "nb_categories_longest_item = get_nb_categories_longest_item_feature()\n",
    "nb_categories_last_item = get_nb_categories_last_item_feature()\n",
    "\n",
    "features = [month, season, day_of_month, weekday, weekend, hour, day_period, night, duration, average_time, length, distinct_nb, most_viewed_item, longest_item, last_item, most_longest_equality, most_last_equality, longest_last_equality, categories_nb, most_present_category_with_count, most_present_category, most_present_category_count, nb_categories_most_viewed_item, nb_categories_longest_item, nb_categories_last_item]\n",
    "\n",
    "test_rdd = features[0].join(features[1])\n",
    "\n",
    "for i in range(2, len(features)):\n",
    "    feature = features[i]\n",
    "    test_rdd = test_rdd.join(feature).mapValues(lambda x: list(x[0])+[x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we transform our RDD to a dataframe which can be used by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_collector(x):\n",
    "    dico = {}\n",
    "    dico[\"session_id\"] = x[0]\n",
    "    for i in range(len(features_names)):\n",
    "        dico[features_names[i]] = x[1][i]\n",
    "    return dico\n",
    "\n",
    "\n",
    "test_df = test_rdd.map(lambda x: Row(**test_collector(x))).toDF()\n",
    "test_df = test_df.fillna(0)\n",
    "test_df = VectorAssembler(inputCols = selected_features_names, outputCol = \"features\").transform(test_df)\n",
    "test_df = test_df.select([\"features\", \"session_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new decision tree model that will be trained on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionTree = DecisionTreeClassifier(maxDepth=2)\n",
    "model = decisionTree.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the list of items which should be predicted for the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_items = candidate_items.rdd.map(lambda x: x[\"item_id\"]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we gather all the results we computed and save them to a CSV file with the structures asked by the RecSys Challenge 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_to_index = dict()\n",
    "\n",
    "df_iterator = indexed_items_df.rdd.toLocalIterator()\n",
    "\n",
    "for row in df_iterator:\n",
    "    item_to_index[row[\"item_id\"]] = int(row[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progressbar = IntProgress(min=0, max=pred.count())\n",
    "display(progressbar)\n",
    "\n",
    "pred_iterator = pred.rdd.toLocalIterator()\n",
    "session_id = 1\n",
    "with open(\"RecSys2022PredictionsLeaderboard.csv\", \"w\") as file:\n",
    "    file.write(\"session_id,item_id,rank\\n\")\n",
    "    for row in pred_iterator:\n",
    "        \n",
    "        temp = list(row[\"probability\"])\n",
    "\n",
    "        temp = [(temp[item_to_index[i]], i) for i in possible_items]\n",
    "        temp.sort(reverse=True)\n",
    "        \n",
    "        temp = temp[:100]\n",
    "        for l in range(100):\n",
    "            file.write(f\"{row['session_id']},{temp[l][1]},{l+1}\\n\")\n",
    "        \n",
    "        progressbar.value += 1\n",
    "        session_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## Video presentation : \n",
    "\n",
    "Please find the presentation of our project using the following link : https://drive.google.com/file/d/1XA5euCfwmq0Yp1CqiVUHU_7ZTO-Mpx-u/view?usp=sharing\n",
    "\n",
    "## Issues with the cluster\n",
    "As already reported by other students, the issues with the cluster really impacted our development. We weren't able to connect from our IDE to the cluster directly so we had to develop our code with very basic autocompletion on the jupyter notebook available on the cluster. This meant we couldn't work in parallel on different cells seemlessly, we had to work separately and merge our modifications which added quite an overhead to the work performance.\n",
    "\n",
    "Also, and most importantly, the issue of availability of the cluster really impacted us hard, totalling for around 25% of our time spent developing. This means we, on average for each session of working on the project, had to dedicate 25% of our time to solve diverse issues with the cluster (slow logging in and access, restarting kernel, saving the project issue, creating backups, etc etc ...). Actually, we believe it would have been faster to not use the cluster at all and to run the computation locally on our computer. The computation would have taken longer but there would have been less friction when working on the project.\n",
    "\n",
    "Finally, when one person is running the notebook and another student try to join the notebook, there is a message asking the other student to either erase all the changes the first student made, or reload the notebook, even without changing anything on the notebook which might have lead to data being lost.\n",
    "\n",
    "\n",
    "\n",
    "## Feature engineered dataset\n",
    "From the multiple .csv file that were available to us for the project, we developed a dataset containing 24 features described in the report. We will enumerate them here as well\n",
    "\n",
    "**Date related**: Month, Season, Day of month, Weekday, Weekend, Hour, Day period, Night, Duration of the session, Average time between consecutive item views, Number of items, Number of distinct items, Most viewed item.\n",
    "\n",
    "**Session related**: Number of items, Number of distinct items.\n",
    "\n",
    "**Item related**: Most viewed item, Longest  viewed item, Last viewed item, Is the most viewed item also the longest viewed item ?, Is the most viewed item also the last item ?, Is the longest viewed item also the last item ?.\n",
    "\n",
    "**Categories related**: Most common number of categories of all items seen, Most present category with its count, Number of categories of the most viewed item, Number of categories of the longest viewed item, Number of categories of the last item.\n",
    "\n",
    "\n",
    "Here is an example of the first few rows of the dataset in a table\n",
    "\n",
    "| session_id | item_id | month | season | day_of_month | weekday | weekend | hour | day_period | night | duration | average_time | length | distinct_nb | most_viewed_item | longest_item | last_item | most_longest_equality | most_last_equality | longest_last_equality | categories_nb | most_present_category | most_present_category_count | nb_categories_most_viewed_item | nb_categories_longest_item | nb_categories_last_item | \n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | \n",
    "| 129978 | 13269 | 4 | 1 | 28 | 3 | 0 | 23 | 3 | 1 | 149 | 30 | 6 | 4 | 1373 | 20271 | 1373 | 0 | 1 | 0 | 25 | 3793 | 6 | 25 | 25 | 25 | \n",
    "| 144420 | 4400 | 5 | 1 | 2 | 6 | 1 | 13 | 1 | 0 | 53 | 53 | 2 | 1 | 16065 | 16065 | 16065 | 1 | 1 | 1 | 27 | 3793 | 2 | 27 | 27 | 27 | \n",
    "| 163676 | 7729 | 1 | 0 | 15 | 5 | 1 | 12 | 3 | 1 | 499 | 125 | 5 | 4 | 4222 | 5966 | 4222 | 0 | 1 | 0 | 24 | 3793 | 3 | 24 | 18 | 24 | \n",
    "| 187746 | 2888 | 5 | 1 | 23 | 0 | 0 | 18 | 3 | 1 | 3672 | 367 | 11 | 8 | 2499 | 9246 | 2499 | 0 | 1 | 0 | 25 | 3793 | 10 | 25 | 25 | 25 | \n",
    "| 264770 | 18969 | 5 | 1 | 30 | 0 | 0 | 18 | 3 | 1 | 211 | 30 | 8 | 6 | 15777 | 26163 | 3237 | 0 | 0 | 0 | 24 | 3793 | 8 | 24 | 24 | 24 | \n",
    "\n",
    "## Feature selection\n",
    "\n",
    "To achieve this dataset used 2 different feature selection algorithms : a feature ranking algorithm as well as a forward feature selection algorithm.\n",
    "\n",
    "For the ranking algorithm, we used the definition available in Pr. Bontempi's book : \"Statistical foundations of machine learning\", and we chose to use mutual information as a univariate measure.\n",
    "\n",
    "For the forward feature selection algorithm, we used the definition available in the same book, and we chose the decision tree classifier to compute the generalisation error as it is the same model that was chosen in the following part.\n",
    "\n",
    "As we could not get the forward feature algorithm to finish in a resonable amount of time due to the cluster, we decided to use results of the ranking algorithm to select the best features. We decided to get rid of the 12 worst features leaving us with this dataset\n",
    "\n",
    "\n",
    "| session_id | item_id | duration | last_item | longest_item | most_viewed_item | average_time | month | nb_categories_last_item | categories_nb | nb_categories_most_viewed_item | season | nb_categories_longest_item | most_present_category | \n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | \n",
    "| 129978 | 13269 | 149 | 1373 | 20271 | 1373 | 30 | 4 | 25 | 25 | 25 | 1 | 25 | 3793 | \n",
    "| 144420 | 4400 | 53 | 16065 | 16065 | 16065 | 53 | 5 | 27 | 27 | 27 | 1 | 27 | 3793 | \n",
    "| 163676 | 7729 | 499 | 4222 | 5966 | 4222 | 125 | 1 | 24 | 24 | 24 | 0 | 18 | 3793 | \n",
    "| 187746 | 2888 | 3672 | 2499 | 9246 | 2499 | 367 | 5 | 25 | 25 | 25 | 1 | 25 | 3793 | \n",
    "| 264770 | 18969 | 211 | 3237 | 26163 | 15777 | 30 | 5 | 24 | 24 | 24 | 1 | 24 | 3793 | \n",
    "\n",
    "\n",
    "\n",
    "## Model\n",
    "\n",
    "We experimented with different models but the one we found the most suitable was the DecisionTreeClassifier from pyspark.ml.classification. \n",
    "\n",
    "When performing a 4-fold cross validation, the model obtained arround 2% of accuracy. This accuracy was not computed using latest state of our project due to the impossibility of running it on the cluster. This result is excellent compared to random picking which has a 2e-4 (0.0002). Therefore, we predict the data 100 times better than what we would have at random.\n",
    "\n",
    "\n",
    "This model was able to generate the prediction for us to submit on the challenge website, where we got a whopping score of 0.022 (2.2%)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
