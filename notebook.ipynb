{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# INTRODUCTION\n",
    "In this notebook we will be using the following packages:\n",
    "  * [pyspark](https://spark.apache.org/docs/latest/api/python/pyspark.html)\n",
    "  * [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
    "  * [pyarrow](https://arrow.apache.org/docs/python/index.html)\n",
    "\n",
    "We will be using the following data:\n",
    "  * dressipi_recsys2022\n",
    "\n",
    "The dataset is split in 4 csv files:\n",
    "   * candidate_items : contains the candidate items for each user\n",
    "   * item_features : contains the features for each item (items can have multiple features)\n",
    "   * train_purchases : contains the purchases for each user\n",
    "   * train_sessions : contains the sessions for each user\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "# Uncomment to install the required packages\n",
    "# %pip install pyspark\n",
    "# %pip install pyarrow\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "\n",
    "# start spark session\n",
    "spark = SparkSession.builder.appName(\"Python Spark SQL basic example\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "\n",
    "candidate_items, item_features, train_purchases, train_sessions = [None] * 4\n",
    "datasets = [candidate_items, item_features, train_purchases, train_sessions]\n",
    "\n",
    "#load all datasets\n",
    "def load_candidate_items():\n",
    "    global candidate_items\n",
    "    candidate_items = spark.read.csv(\"dressipi_recsys2022/candidate_items.csv\", header=True)\n",
    "    candidate_items = candidate_items.withColumn(\"item_id\", candidate_items[\"item_id\"].cast(\"int\"))\n",
    "\n",
    "def load_item_features():\n",
    "    global item_features\n",
    "    item_features = spark.read.csv(\"dressipi_recsys2022/item_features.csv\", header=True)\n",
    "    item_features = item_features.withColumn(\"item_id\", item_features[\"item_id\"].cast(\"int\"))\n",
    "    item_features = item_features.withColumn(\"feature_category_id\", item_features[\"feature_category_id\"].cast(\"int\"))\n",
    "    item_features = item_features.withColumn(\"feature_value_id\", item_features[\"feature_value_id\"].cast(\"int\"))\n",
    "\n",
    "def load_train_purchases():\n",
    "    global train_purchases\n",
    "    train_purchases = spark.read.csv(\"dressipi_recsys2022/train_purchases.csv\", header=True)\n",
    "    train_purchases = train_purchases.withColumn(\"session_id\", train_purchases[\"session_id\"].cast(\"int\"))\n",
    "    train_purchases = train_purchases.withColumn(\"item_id\", train_purchases[\"item_id\"].cast(\"int\"))\n",
    "    train_purchases = train_purchases.withColumn(\"date\", train_purchases[\"date\"].cast(\"timestamp\"))\n",
    "\n",
    "def load_train_sessions():\n",
    "    global train_sessions\n",
    "    train_sessions = spark.read.csv(\"dressipi_recsys2022/train_sessions.csv\", header=True)\n",
    "    train_sessions = train_sessions.withColumn(\"session_id\", train_sessions[\"session_id\"].cast(\"int\"))\n",
    "    train_sessions = train_sessions.withColumn(\"item_id\", train_sessions[\"item_id\"].cast(\"int\"))\n",
    "    train_sessions = train_sessions.withColumn(\"date\", train_sessions[\"date\"].cast(\"timestamp\"))\n",
    "\n",
    "def load_datasets():\n",
    "    global datasets\n",
    "    load_candidate_items()\n",
    "    load_item_features()\n",
    "    load_train_purchases()\n",
    "    load_train_sessions()\n",
    "    datasets = [candidate_items, item_features, train_purchases, train_sessions]\n",
    "\n",
    "load_datasets()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quick look at the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### train_sessions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      "\n",
      "+----------+-------+--------------------+\n",
      "|session_id|item_id|                date|\n",
      "+----------+-------+--------------------+\n",
      "|         3|   9655|2020-12-18 21:25:...|\n",
      "|         3|   9655|2020-12-18 21:19:...|\n",
      "|        13|  15654|2020-03-13 19:35:...|\n",
      "|        18|  18316|2020-08-26 19:18:...|\n",
      "|        18|   2507|2020-08-26 19:16:...|\n",
      "+----------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_sessions.printSchema()\n",
    "train_sessions.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### train_purchases"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+\n",
      "|session_id|item_id|                date|\n",
      "+----------+-------+--------------------+\n",
      "|         3|  15085|2020-12-18 21:26:...|\n",
      "|        13|  18626|2020-03-13 19:36:...|\n",
      "|        18|  24911|2020-08-26 19:20:...|\n",
      "|        19|  12534|2020-11-02 17:16:...|\n",
      "|        24|  13226|2020-02-26 18:27:...|\n",
      "+----------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_purchases.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### item_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+\n",
      "|item_id|feature_category_id|feature_value_id|\n",
      "+-------+-------------------+----------------+\n",
      "|      2|                 56|             365|\n",
      "|      2|                 62|             801|\n",
      "|      2|                 68|             351|\n",
      "|      2|                 33|             802|\n",
      "|      2|                 72|              75|\n",
      "+-------+-------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_features.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### candidate_items"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|item_id|\n",
      "+-------+\n",
      "|      4|\n",
      "|      8|\n",
      "|      9|\n",
      "|     19|\n",
      "|     20|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "candidate_items.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1 : Pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checking for null values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate_items: 0\n",
      "item_features: 0\n",
      "train_purchases: 0\n",
      "train_sessions: 0\n"
     ]
    }
   ],
   "source": [
    "# Are there missing values (na or null) in any of those dataframes ? print the number of missing values\n",
    "print(\"candidate_items:\", candidate_items.filter(candidate_items[\"item_id\"].isNull()).count())\n",
    "print(\"item_features:\", item_features.filter(item_features[\"item_id\"].isNull()).count())\n",
    "print(\"train_purchases:\", train_purchases.filter(train_purchases[\"item_id\"].isNull()).count())\n",
    "print(\"train_sessions:\", train_sessions.filter(train_sessions[\"item_id\"].isNull()).count())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature engineering"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1) Month"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(24, 2), (48, 4), (184, 4), (208, 12), (232, 9)]\n"
     ]
    }
   ],
   "source": [
    "month = train_sessions.rdd.map(lambda x: (x[\"session_id\"], int(x[\"date\"].strftime(\"%m\")))).reduceByKey(min)\n",
    "print(month.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2) Day of month"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(24, 26), (48, 15), (184, 4), (208, 13), (232, 11)]\n"
     ]
    }
   ],
   "source": [
    "day_of_month = train_sessions.rdd.map(lambda x: (x[\"session_id\"], int(x[\"date\"].strftime(\"%d\")))).reduceByKey(min)\n",
    "print(day_of_month.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3) Weekday"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(24, 3), (48, 3), (184, 0), (208, 0), (232, 5)]\n"
     ]
    }
   ],
   "source": [
    "weekday = train_sessions.rdd.map(lambda x: (x[\"session_id\"], int(x[\"date\"].strftime(\"%w\")))).reduceByKey(min)\n",
    "print(weekday.take(5))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4) Hour period"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(24, 17), (48, 17), (184, 1), (208, 9), (232, 12)]\n"
     ]
    }
   ],
   "source": [
    "hour_period = train_sessions.rdd.map(lambda x: (x[\"session_id\"], int(x[\"date\"].strftime(\"%H\")))).reduceByKey(min)\n",
    "print(hour_period.take(5))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5) Season (Meteorological)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(24, 0), (48, 1), (184, 1), (208, 0), (232, 3)]\n"
     ]
    }
   ],
   "source": [
    "def get_season(month):\n",
    "    if month == 12 or month <= 2: return 0\n",
    "    elif 2 < month <= 5: return 1\n",
    "    elif 5 < month <= 8: return 2\n",
    "    elif 8 < month <= 11: return 3\n",
    "\n",
    "season = train_sessions.rdd.map(lambda x: (x[\"session_id\"], get_season(int(x[\"date\"].strftime(\"%m\"))))).reduceByKey(min)\n",
    "print(season.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "6) Average time between consecutive item views"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "[(24, datetime.timedelta(seconds=462, microseconds=983375)),\n (48, datetime.timedelta(seconds=657, microseconds=103000)),\n (184, datetime.timedelta(0)),\n (208, datetime.timedelta(0)),\n (232, datetime.timedelta(0))]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_average_time(dates):\n",
    "    import datetime\n",
    "    dates = sorted(list(dates))\n",
    "    avgs = [dates[i+1] - dates[i] for i in range(len(dates)-1)]\n",
    "    return sum(avgs, datetime.timedelta())/len(avgs) if len(avgs) > 0 else datetime.timedelta(0)\n",
    "\n",
    "average_time = train_sessions.rdd.map(lambda x: (x[\"session_id\"], x[\"date\"])).groupByKey().mapValues(get_average_time)\n",
    "\n",
    "average_time.take(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "7) Number of distinct items"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "[(24, 8), (48, 2), (184, 1), (208, 1), (232, 1)]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_nb = train_sessions.rdd.map(lambda x: (x[\"session_id\"], x[\"item_id\"])).groupByKey().mapValues(lambda x: len(set(x)))\n",
    "distinct_nb.take(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "8) Number of repetitive items"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "[(24, 1), (48, 0), (184, 0), (208, 0), (232, 0)]"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repetitive_nb = train_sessions.rdd.map(lambda x: (x[\"session_id\"], x[\"item_id\"])).groupByKey().mapValues(lambda x: len(list(x)) - len(set(x)))\n",
    "repetitive_nb.take(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "9) Same category"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "[(2,\n  [(56, 365),\n   (62, 801),\n   (68, 351),\n   (33, 802),\n   (72, 75),\n   (29, 123),\n   (16, 38),\n   (50, 76),\n   (61, 462),\n   (53, 6),\n   (7, 394),\n   (69, 885),\n   (47, 123)]),\n (4,\n  [(55, 267),\n   (17, 378),\n   (5, 605),\n   (69, 538),\n   (18, 289),\n   (68, 373),\n   (50, 317),\n   (73, 544),\n   (65, 521),\n   (7, 837),\n   (59, 180),\n   (47, 218),\n   (46, 825),\n   (56, 365),\n   (4, 618),\n   (26, 268),\n   (3, 793),\n   (19, 148),\n   (32, 902),\n   (63, 861),\n   (22, 881),\n   (61, 462),\n   (45, 559),\n   (72, 75)]),\n (8,\n  [(56, 365),\n   (55, 267),\n   (7, 798),\n   (69, 592),\n   (72, 75),\n   (3, 793),\n   (45, 559),\n   (18, 817),\n   (59, 856),\n   (73, 544),\n   (5, 605),\n   (61, 462),\n   (68, 351),\n   (4, 618),\n   (65, 521),\n   (26, 268),\n   (50, 317),\n   (32, 902),\n   (46, 825),\n   (17, 378),\n   (19, 700),\n   (11, 735),\n   (63, 861),\n   (47, 549)]),\n (10,\n  [(56, 365),\n   (15, 451),\n   (63, 816),\n   (12, 410),\n   (30, 564),\n   (61, 706),\n   (50, 128),\n   (69, 639),\n   (7, 798),\n   (30, 482),\n   (72, 75),\n   (62, 801),\n   (68, 531),\n   (21, 353),\n   (47, 96)]),\n (14,\n  [(61, 706),\n   (14, 857),\n   (8, 636),\n   (70, 81),\n   (32, 101),\n   (50, 76),\n   (34, 275),\n   (16, 38),\n   (7, 414),\n   (26, 268),\n   (69, 592),\n   (5, 472),\n   (55, 704),\n   (57, 233),\n   (56, 365),\n   (25, 376),\n   (47, 842),\n   (72, 219),\n   (68, 379)])]"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features_rdd = item_features.rdd.map(lambda x: (x[\"item_id\"], (x[\"feature_category_id\"], x[\"feature_value_id\"]))).groupByKey().mapValues(lambda x: [(a,b) for a, b in x])\n",
    "\n",
    "item_features_rdd.take(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "data": {
      "text/plain": "[(41200, 53), (198620, 0), (427520, 56), (639330, 83), (688110, 42)]"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_same_category(x):\n",
    "    dico = dict()\n",
    "    for item in x:\n",
    "        for cat in item:\n",
    "            if cat in dico:\n",
    "                dico[cat] += 1\n",
    "            else:\n",
    "                dico[cat] = 0\n",
    "\n",
    "    res = 0\n",
    "    for val in dico.values():\n",
    "        if val > 0:\n",
    "            res += 1\n",
    "\n",
    "    return res\n",
    "\n",
    "same_category = item_features_rdd.join(train_sessions.rdd.map(lambda x: (x[\"item_id\"], x[\"session_id\"]))).map(lambda x: (x[1][1], x[1][0])).groupByKey().mapValues(get_same_category)\n",
    "same_category.take(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "10) Different category"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "data": {
      "text/plain": "[(41200, 30), (198620, 24), (427520, 19), (639330, 26), (688110, 47)]"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_different_category(x):\n",
    "    dico = dict()\n",
    "    for item in x:\n",
    "        for cat in item:\n",
    "            if cat in dico:\n",
    "                dico[cat] += 1\n",
    "            else:\n",
    "                dico[cat] = 0\n",
    "\n",
    "    res = 0\n",
    "    for val in dico.values():\n",
    "        if val == 0:\n",
    "            res += 1\n",
    "\n",
    "    return res\n",
    "\n",
    "diff_category = item_features_rdd.join(train_sessions.rdd.map(lambda x: (x[\"item_id\"], x[\"session_id\"]))).map(lambda x: (x[1][1], x[1][0])).groupByKey().mapValues(get_different_category)\n",
    "diff_category.take(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "11) Last item"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "[(24, 18476), (48, 26404), (184, 14383), (208, 26257), (232, 19464)]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_last_item(items):\n",
    "    return max(list(items), key=lambda i: i[1])[0]\n",
    "\n",
    "last_item = train_sessions.rdd.map(lambda x: (x[\"session_id\"], (x[\"item_id\"], x[\"date\"]))).groupByKey().mapValues(lambda x: max(x, key=lambda i: i[1])[0])\n",
    "last_item.take(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2 : Feature selection\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ranking algorithm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Pearson correlation\n",
    "\n",
    "def pearson_reduce(a, b):\n",
    "    a, b = a[1], b[1]\n",
    "\n",
    "    n = a[0] + b[0]\n",
    "    x = a[1] + b[1]\n",
    "    y = a[2] + b[2]\n",
    "    x2 = a[3] + b[3]\n",
    "    y2 = a[4] + b[4]\n",
    "    xy = a[5] + b[5]\n",
    "\n",
    "    return 0, (n, x, y, x2, y2, xy)\n",
    "\n",
    "def calculate_pearson(a):\n",
    "    import math\n",
    "\n",
    "    n = a[0]\n",
    "    x = a[1]\n",
    "    y = a[2]\n",
    "    x2 = a[3]\n",
    "    y2 = a[4]\n",
    "    xy = a[5]\n",
    "\n",
    "    return (n*xy - x * y) / (math.sqrt((n*x2 - (x**2)) * (n*y2 - (y**2))))\n",
    "\n",
    "test = season.join(season)\n",
    "\n",
    "d = test.mapValues(lambda a: (1, a[0], a[1], a[0]**2, a[1]**2, a[0]*a[1])).reduce(pearson_reduce)[1]\n",
    "print(calculate_pearson(d))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Forward feature selection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 3 : Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "data": {
      "text/plain": "[2, 4]"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[Row(item_id=2, feature_category_id=56, feature_value_id=365), Row(item_id=2, feature_category_id=62, feature_value_id=801), Row(item_id=2, feature_category_id=68, feature_value_id=351), Row(item_id=2, feature_category_id=33, feature_value_id=802), Row(item_id=2, feature_category_id=72, feature_value_id=75), Row(item_id=2, feature_category_id=29, feature_value_id=123), Row(item_id=2, feature_category_id=16, feature_value_id=38), Row(item_id=2, feature_category_id=50, feature_value_id=76), Row(item_id=2, feature_category_id=61, feature_value_id=462), Row(item_id=2, feature_category_id=53, feature_value_id=6), Row(item_id=2, feature_category_id=7, feature_value_id=394), Row(item_id=2, feature_category_id=69, feature_value_id=885), Row(item_id=2, feature_category_id=47, feature_value_id=123)]\n",
      "[]\n",
      "[Row(session_id=3401213, item_id=2, date=datetime.datetime(2021, 3, 28, 8, 24, 15, 321000))]\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Forward feature selection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 3 : Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show rows with item_id == 2 in all dataframes\n",
    "for d in datasets:\n",
    "    print(d.filter(d[\"item_id\"] == 2).collect())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}