{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# INTRODUCTION\n",
    "In this notebook we will be using the following packages:\n",
    "  * [pyspark](https://spark.apache.org/docs/latest/api/python/pyspark.html)\n",
    "  * [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
    "  * [pyarrow](https://arrow.apache.org/docs/python/index.html)\n",
    "\n",
    "We will be using the following data:\n",
    "  * dressipi_recsys2022\n",
    "\n",
    "The dataset is split in 4 csv files:\n",
    "   * candidate_items : contains the candidate items for each user\n",
    "   * item_features : contains the features for each item (items can have multiple features)\n",
    "   * train_purchases : contains the purchases for each user\n",
    "   * train_sessions : contains the sessions for each user\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to install the required packages\n",
    "# %pip install pyspark\n",
    "# %pip install pyarrow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# start spark session\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=2g  pyspark-shell\"\n",
    "# spark = SparkSession.builder.appName(\"Python Spark SQL basic example\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"AppName\") \\\n",
    "    .config('spark.ui.port', '4050')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "candidate_items, item_features, train_purchases, train_sessions = [None] * 4\n",
    "datasets = [candidate_items, item_features, train_purchases, train_sessions]\n",
    "\n",
    "#load all datasets\n",
    "def load_candidate_items():\n",
    "    global candidate_items\n",
    "    candidate_items = spark.read.csv(\"dressipi_recsys2022/candidate_items.csv\", header=True)\n",
    "    candidate_items = candidate_items.withColumn(\"item_id\", candidate_items[\"item_id\"].cast(\"int\"))\n",
    "\n",
    "def load_item_features():\n",
    "    global item_features\n",
    "    item_features = spark.read.csv(\"dressipi_recsys2022/item_features.csv\", header=True)\n",
    "    item_features = item_features.withColumn(\"item_id\", item_features[\"item_id\"].cast(\"int\"))\n",
    "    item_features = item_features.withColumn(\"feature_category_id\", item_features[\"feature_category_id\"].cast(\"int\"))\n",
    "    item_features = item_features.withColumn(\"feature_value_id\", item_features[\"feature_value_id\"].cast(\"int\"))\n",
    "\n",
    "def load_train_purchases():\n",
    "    global train_purchases\n",
    "    train_purchases = spark.read.csv(\"dressipi_recsys2022/train_purchases.csv\", header=True)\n",
    "    train_purchases = train_purchases.withColumn(\"session_id\", train_purchases[\"session_id\"].cast(\"int\"))\n",
    "    train_purchases = train_purchases.withColumn(\"item_id\", train_purchases[\"item_id\"].cast(\"int\"))\n",
    "    train_purchases = train_purchases.withColumn(\"date\", train_purchases[\"date\"].cast(\"timestamp\"))\n",
    "\n",
    "def load_train_sessions():\n",
    "    global train_sessions\n",
    "    train_sessions = spark.read.csv(\"dressipi_recsys2022/train_sessions.csv\", header=True)\n",
    "    train_sessions = train_sessions.withColumn(\"session_id\", train_sessions[\"session_id\"].cast(\"int\"))\n",
    "    train_sessions = train_sessions.withColumn(\"item_id\", train_sessions[\"item_id\"].cast(\"int\"))\n",
    "    train_sessions = train_sessions.withColumn(\"date\", train_sessions[\"date\"].cast(\"timestamp\"))\n",
    "\n",
    "def load_datasets():\n",
    "    global datasets\n",
    "    load_candidate_items()\n",
    "    load_item_features()\n",
    "    load_train_purchases()\n",
    "    load_train_sessions()\n",
    "    datasets = [candidate_items, item_features, train_purchases, train_sessions]\n",
    "\n",
    "load_datasets()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Quick look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### train_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      "\n",
      "+----------+-------+--------------------+\n",
      "|session_id|item_id|                date|\n",
      "+----------+-------+--------------------+\n",
      "|         3|   9655|2020-12-18 21:25:...|\n",
      "|         3|   9655|2020-12-18 21:19:...|\n",
      "|        13|  15654|2020-03-13 19:35:...|\n",
      "|        18|  18316|2020-08-26 19:18:...|\n",
      "|        18|   2507|2020-08-26 19:16:...|\n",
      "+----------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_sessions.printSchema()\n",
    "train_sessions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### train_purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+\n",
      "|session_id|item_id|                date|\n",
      "+----------+-------+--------------------+\n",
      "|         3|  15085|2020-12-18 21:26:...|\n",
      "|        13|  18626|2020-03-13 19:36:...|\n",
      "|        18|  24911|2020-08-26 19:20:...|\n",
      "|        19|  12534|2020-11-02 17:16:...|\n",
      "|        24|  13226|2020-02-26 18:27:...|\n",
      "+----------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_purchases.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+\n",
      "|item_id|feature_category_id|feature_value_id|\n",
      "+-------+-------------------+----------------+\n",
      "|      2|                 56|             365|\n",
      "|      2|                 62|             801|\n",
      "|      2|                 68|             351|\n",
      "|      2|                 33|             802|\n",
      "|      2|                 72|              75|\n",
      "+-------+-------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### candidate_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|item_id|\n",
      "+-------+\n",
      "|      4|\n",
      "|      8|\n",
      "|      9|\n",
      "|     19|\n",
      "|     20|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "candidate_items.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 1 : Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate_items: 0\n",
      "item_features: 0\n",
      "train_purchases: 0\n",
      "train_sessions: 0\n"
     ]
    }
   ],
   "source": [
    "# Are there missing values (na or null) in any of those dataframes ? print the number of missing values\n",
    "print(\"candidate_items:\", candidate_items.filter(candidate_items[\"item_id\"].isNull()).count())\n",
    "print(\"item_features:\", item_features.filter(item_features[\"item_id\"].isNull()).count())\n",
    "print(\"train_purchases:\", train_purchases.filter(train_purchases[\"item_id\"].isNull()).count())\n",
    "print(\"train_sessions:\", train_sessions.filter(train_sessions[\"item_id\"].isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_date_rdd = train_sessions.rdd.map(lambda x: (x[\"session_id\"], x[\"date\"])).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1) Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(18, 8), (24, 2), (28, 5), (36, 6), (42, 3)]\n"
     ]
    }
   ],
   "source": [
    "month = session_date_rdd.mapValues(lambda x: int(x.strftime(\"%m\"))).reduceByKey(min)\n",
    "print(month.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2) Season (Meteorological)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(18, 2), (24, 0), (28, 1), (36, 2), (42, 1)]\n"
     ]
    }
   ],
   "source": [
    "def get_season(month):\n",
    "    if month == 12 or month <= 2: return 0\n",
    "    elif 2 < month <= 5: return 1\n",
    "    elif 5 < month <= 8: return 2\n",
    "    elif 8 < month <= 11: return 3\n",
    "\n",
    "season = month.mapValues(get_season).reduceByKey(min)\n",
    "print(season.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3) Day of month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(18, 26), (24, 26), (28, 18), (36, 21), (42, 1)]\n"
     ]
    }
   ],
   "source": [
    "day_of_month = session_date_rdd.mapValues(lambda x: int(x.strftime(\"%d\"))).reduceByKey(min)\n",
    "print(day_of_month.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4) Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(18, 3), (24, 3), (28, 1), (36, 0), (42, 1)]\n"
     ]
    }
   ],
   "source": [
    "weekday = session_date_rdd.mapValues(lambda x: int(x.strftime(\"%w\"))).reduceByKey(min)\n",
    "print(weekday.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(18, 0), (24, 0), (28, 0), (36, 0), (42, 0)]\n"
     ]
    }
   ],
   "source": [
    "weekend = weekday.mapValues(lambda x: int(x in (5, 6)))\n",
    "print(weekend.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "6) Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(18, 19), (24, 17), (28, 12), (36, 10), (42, 15)]\n"
     ]
    }
   ],
   "source": [
    "hour = session_date_rdd.mapValues(lambda x: int(x.strftime(\"%H\"))).reduceByKey(min)\n",
    "print(hour_period.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22561, 10068,  6930,  5703,  6069, 11125, 25801, 43049, 55713,\n",
       "       60538, 59598, 58602, 58549, 56888, 54420, 55018, 55260, 56836,\n",
       "       62549, 69150, 67415, 53219, 33136, 11803])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hour.map(lambda x: [int(x[1] == i) for i in range(24)]).reduce(np.add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Day period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(18, 2), (24, 1), (28, 3), (36, 0), (42, 1)]\n"
     ]
    }
   ],
   "source": [
    "def get_day(x):\n",
    "    if 6 < x < 12: return 0\n",
    "    elif 12 < x < 18: return 1\n",
    "    elif 18 < x < 22: return 2\n",
    "    else: return 3\n",
    "    \n",
    "day_period = hour.mapValues(get_day)\n",
    "print(day_period.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(18, 0), (24, 0), (28, 1), (36, 0), (42, 0)]\n"
     ]
    }
   ],
   "source": [
    "night = day_period.mapValues(lambda x: int(x == 3))\n",
    "print(night.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Duration of the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18, 163.601), (24, 3703.867), (28, 87.513), (36, 43.074), (42, 120.348)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_session_duration(dates):\n",
    "    dates = list(dates)\n",
    "    dates.sort()\n",
    "    return (dates[-1] - dates[0]).total_seconds() if len(dates) >= 2 else 0\n",
    "\n",
    "duration = session_date_rdd.groupByKey().mapValues(get_session_duration)\n",
    "print(duration.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "10) Average time between consecutive item views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18, 81.8005), (24, 462.983375), (28, 29.171), (36, 43.074), (42, 40.116)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_average_time(dates):\n",
    "    import datetime\n",
    "    dates = sorted(list(dates))\n",
    "    avgs = [dates[i+1] - dates[i] for i in range(len(dates)-1)]\n",
    "    return (sum(avgs, datetime.timedelta())/len(avgs)).total_seconds() if len(avgs) > 0 else 0\n",
    "\n",
    "average_time = session_date_rdd.groupByKey().mapValues(get_average_time)\n",
    "\n",
    "print(average_time.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_item_rdd = train_sessions.rdd.map(lambda x: (x[\"session_id\"], x[\"item_id\"])).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "11) Number of distinct items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18, 3), (24, 8), (28, 3), (36, 2), (42, 4)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_nb = session_item_rdd.groupByKey().mapValues(lambda x: len(set(x)))\n",
    "print(distinct_nb.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "12) Number of repetitive items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18, 0), (24, 1), (28, 1), (36, 0), (42, 0)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repetitive_nb = session_item_rdd.groupByKey().mapValues(lambda x: len(list(x)) - len(set(x)))\n",
    "print(repetitive_nb.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "13) Same category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2,\n",
       "  [(56, 365),\n",
       "   (62, 801),\n",
       "   (68, 351),\n",
       "   (33, 802),\n",
       "   (72, 75),\n",
       "   (29, 123),\n",
       "   (16, 38),\n",
       "   (50, 76),\n",
       "   (61, 462),\n",
       "   (53, 6),\n",
       "   (7, 394),\n",
       "   (69, 885),\n",
       "   (47, 123)])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features_rdd = item_features.rdd.map(lambda x: (x[\"item_id\"], (x[\"feature_category_id\"], x[\"feature_value_id\"]))).groupByKey().mapValues(lambda x: [(a,b) for a, b in x])\n",
    "print(item_features_rdd.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(809760, 44), (2177144, 35), (2618892, 30), (4350700, 33), (2560368, 59)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_same_category(x):\n",
    "    dico = dict()\n",
    "    for item in x:\n",
    "        for cat in item:\n",
    "            if cat in dico:\n",
    "                dico[cat] += 1\n",
    "            else:\n",
    "                dico[cat] = 0\n",
    "\n",
    "    res = 0\n",
    "    for val in dico.values():\n",
    "        if val > 0:\n",
    "            res += 1\n",
    "\n",
    "    return res\n",
    "\n",
    "same_category = item_features_rdd.join(train_sessions.rdd.map(lambda x: (x[\"item_id\"], x[\"session_id\"]))).map(lambda x: (x[1][1], x[1][0])).groupByKey().mapValues(get_same_category)\n",
    "print(same_category.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "14) Different category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2044264, 47), (118976, 37), (252868, 6), (2082460, 29), (2761440, 30)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_different_category(x):\n",
    "    dico = dict()\n",
    "    for item in x:\n",
    "        for cat in item:\n",
    "            if cat in dico:\n",
    "                dico[cat] += 1\n",
    "            else:\n",
    "                dico[cat] = 0\n",
    "\n",
    "    res = 0\n",
    "    for val in dico.values():\n",
    "        if val == 0:\n",
    "            res += 1\n",
    "\n",
    "    return res\n",
    "\n",
    "diff_category = item_features_rdd.join(train_sessions.rdd.map(lambda x: (x[\"item_id\"], x[\"session_id\"]))).map(lambda x: (x[1][1], x[1][0])).groupByKey().mapValues(get_different_category)\n",
    "print(diff_category.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "15) Last item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(496, 26180), (1074, 6107), (1740, 13117), (2010, 1344), (2934, 20666)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_last_item(items):\n",
    "    return max(list(items), key=lambda i: i[1])[0]\n",
    "\n",
    "last_item = train_sessions.rdd.map(lambda x: (x[\"session_id\"], (x[\"item_id\"], x[\"date\"]))).groupByKey().mapValues(lambda x: max(x, key=lambda i: i[1])[0])\n",
    "print(last_item.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16) Most present category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1287288, (4, 618)),\n",
       " (2642212, (4, 618)),\n",
       " (2927608, (3, 793)),\n",
       " (2060688, (3, 793)),\n",
       " (1518656, (3, 793))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_most_present_category(categories_i):\n",
    "    categories = [cat for cat_i in categories_i for cat in cat_i]    \n",
    "    categories.sort()\n",
    "    \n",
    "    most_viewed = (None, -1)\n",
    "    last_viewed = categories[0]\n",
    "    cnt = 0\n",
    "    \n",
    "    for category in categories:\n",
    "        if last_viewed != category:\n",
    "            if cnt > most_viewed[1]:\n",
    "                most_viewed = (last_viewed, cnt)\n",
    "                cnt = 1\n",
    "                last_viewed = category\n",
    "        else:\n",
    "            cnt += 1\n",
    "            \n",
    "    if cnt > most_viewed[1]:\n",
    "        most_viewed = (last_viewed, cnt)\n",
    "    \n",
    "    return most_viewed[0]\n",
    "    \n",
    "most_present_category = item_features_rdd.join(train_sessions.rdd.map(lambda x: (x[\"item_id\"], x[\"session_id\"]))).map(lambda x: (x[1][1], x[1][0])).groupByKey().mapValues(get_most_present_category)\n",
    "print(most_present_category.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17) Most viewed item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18, 2507), (24, 2927), (28, 11529), (36, 25417), (42, 10395)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_most_viewed_item(items):\n",
    "    items = list(items)\n",
    "    items.sort()\n",
    "    most_viewed = (None, -1)\n",
    "    \n",
    "    last_viewed = items[0]\n",
    "    cnt = 0\n",
    "    \n",
    "    for item in items:\n",
    "        if last_viewed != item:\n",
    "            if cnt > most_viewed[1]:\n",
    "                most_viewed = (last_viewed, cnt)\n",
    "                cnt = 1\n",
    "                last_viewed = item\n",
    "        else:\n",
    "            cnt += 1\n",
    "            \n",
    "    if cnt > most_viewed[1]:\n",
    "        most_viewed = (last_viewed, cnt)\n",
    "    \n",
    "    return most_viewed[0]\n",
    "    \n",
    "most_viewed_item = session_item_rdd.groupByKey().mapValues(get_most_viewed_item)\n",
    "print(most_viewed_item.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18) Length of the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18, 3), (24, 9), (28, 4), (36, 2), (42, 4)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = session_item_rdd.groupByKey().mapValues(len)\n",
    "print(length.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19) Longest viewed item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18, 2507), (24, 2927), (28, 16895), (36, 26536), (42, 20523)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_longest_item(items):\n",
    "    items = list(items)\n",
    "    t = [items[i+1][1] - items[i][1] for i in range(len(items)-1)]\n",
    "    return items[np.argmax(t)][0] if len(t) > 0 else items[0][0]\n",
    "\n",
    "longest_item = train_sessions.rdd.map(lambda x: (x[\"session_id\"], (x[\"item_id\"], x[\"date\"]))).\\\n",
    "                    groupByKey().\\\n",
    "                    mapValues(get_longest_item)\n",
    "print(longest_item.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20) Median number of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7588, 24), (9396, 21.0), (28268, 18), (59544, 25.0), (61536, 24)]\n"
     ]
    }
   ],
   "source": [
    "def get_categories_nb(cat):\n",
    "    from statistics import median\n",
    "    nb = [len(c) for c in cat]\n",
    "    return median(nb)\n",
    "    \n",
    "categories_nb = item_features_rdd.join(train_sessions.rdd.map(lambda x: (x[\"item_id\"], x[\"session_id\"]))).map(lambda x: (x[1][1], x[1][0])).groupByKey().mapValues(get_categories_nb)\n",
    "print(categories_nb.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21) Hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(18, -4836734901686818560), (24, -5971015408950527618), (28, -6017024854127706481), (36, 8998393808888012635), (42, 6148542230826542357)]\n"
     ]
    }
   ],
   "source": [
    "hash_items = session_item_rdd.groupByKey().mapValues(lambda x: hash(str(list(x))))\n",
    "print(hash_items.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "BIG RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "session_item_id = train_purchases.rdd.map(lambda x: (x[\"session_id\"], x[\"item_id\"]))\n",
    "# features = [month, season, day_of_month, weekday, weekend, hour, day_period, night, duration, average_time, distinct_nb, repetitive_nb, same_category, diff_category, last_item, most_present_category, most_viewed_item, length, longest_item, categories_nb, hash_items]\n",
    "# features_name = [\"month\", \"season\", \"day_of_month\", \"weekday\", \"weekend\", \"hour\", \"day_period\", \"night\", \"duration\", \"average_time\", \"distinct_nb\", \"repetitive_nb\", \"same_category\", \"diff_category\", \"last_item\", \"most_present_category\", \"most_viewed_item\", \"length\", \"longest_item\", \"categories_nb\", \"hash_items\"]\n",
    "\n",
    "features = [month, season, day_of_month, weekday, weekend, hour, day_period, night, duration, average_time, distinct_nb, repetitive_nb, same_category, diff_category, last_item, most_viewed_item, length, longest_item, categories_nb, hash_items]\n",
    "features_name = [\"month\", \"season\", \"day_of_month\", \"weekday\", \"weekend\", \"hour\", \"day_period\", \"night\", \"duration\", \"average_time\", \"distinct_nb\", \"repetitive_nb\", \"same_category\", \"diff_category\", \"last_item\", \"most_viewed_item\", \"length\", \"longest_item\", \"categories_nb\", \"hash_items\"]\n",
    "\n",
    "\n",
    "def get_BIG_RDD():\n",
    "    temp = session_item_id\n",
    "    for feature in features:\n",
    "        temp = temp.join(feature).mapValues(lambda x: tuple(list(x[0])+[x[1]]) if isinstance(x[0], tuple) else x)\n",
    "    return temp\n",
    "\n",
    "BIG_RDD = get_BIG_RDD().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(966000, (24101, 5, 1, 14, 5, 1, 7, 0, 0, 136.702, 68.351, 3, 0, 15, 21, 23875, 1808, 3, 18876, 19, 7261787585238309112))]\n"
     ]
    }
   ],
   "source": [
    "print(BIG_RDD.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 2 : Feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Ranking algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pearson_reduce(a, b):\n",
    "    n = a[0] + b[0]\n",
    "    x = a[1] + b[1]\n",
    "    y = a[2] + b[2]\n",
    "    x2 = a[3] + b[3]\n",
    "    y2 = a[4] + b[4]\n",
    "    xy = a[5] + b[5]\n",
    "\n",
    "    return n, x, y, x2, y2, xy\n",
    "\n",
    "def calculate_pearson(a):\n",
    "    import math\n",
    "\n",
    "    n = a[0]\n",
    "    x = a[1]\n",
    "    y = a[2]\n",
    "    x2 = a[3]\n",
    "    y2 = a[4]\n",
    "    xy = a[5]\n",
    "\n",
    "    return (n*xy - x * y) / (math.sqrt((n*x2 - (x**2)) * (n*y2 - (y**2))))\n",
    "\n",
    "def compute_pearson(rdd):\n",
    "    temp = rdd.flatMap(lambda x: [(i, (x[1][i+1], x[1][0])) for i in range(len(x[1])-1)]).\\\n",
    "        mapValues(lambda a: (1, a[0], a[1], a[0]**2, a[1]**2, a[0]*a[1])).\\\n",
    "        reduceByKey(pearson_reduce).\\\n",
    "        mapValues(calculate_pearson).collect()\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features ranking:\n",
      "\t- categories_nb: -0.007708906072711242\n",
      "\t- month: -0.0071614357553861745\n",
      "\t- day_of_month: -0.006264159623711706\n",
      "\t- season: -0.004767394584903107\n",
      "\t- length: 0.002930539306274257\n",
      "\t- distinct_nb: 0.0027738942053009237\n",
      "\t- same_category: 0.0025279332476597477\n",
      "\t- most_viewed_item: -0.0024521005141891605\n",
      "\t- weekend: -0.0022594336768649668\n",
      "\t- repetitive_nb: 0.002235108828861097\n",
      "\t- hash_items: -0.0021588631256048373\n",
      "\t- diff_category: -0.0020495355809238697\n",
      "\t- weekday: -0.001951938823723691\n",
      "\t- duration: 0.0016147845578619738\n",
      "\t- longest_item: -0.00159280811850096\n",
      "\t- last_item: -0.0014335263673236035\n",
      "\t- night: -0.0006003490621040677\n",
      "\t- average_time: 0.0005275049188534646\n",
      "\t- hour: 0.00014790122764263172\n",
      "\t- day_period: -0.00013857527522151224\n"
     ]
    }
   ],
   "source": [
    "features_score = compute_pearson(BIG_RDD)\n",
    "features_score.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(\"Features ranking:\")\n",
    "for feat in features_score:\n",
    "    print(f\"\\t- {features_name[feat[0]]}: {feat[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST Cramer's V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "\n",
    "def f(x):\n",
    "    dico = {}\n",
    "    dico[\"session_id\"] = x[0]\n",
    "    dico[\"item_id\"] = x[1][0]\n",
    "    for i in range(len(features_name)):\n",
    "        dico[features_name[i]] = x[1][i+1]\n",
    "    return dico\n",
    "        \n",
    "        \n",
    "df = BIG_RDD.map(lambda x: Row(**f(x))).toDF().toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hash_items\"] = np.abs(df[\"hash_items\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month : 0.037410992802345576\n",
      "season : 0.020655312197883156\n",
      "day_of_month : 0.06079113289868695\n",
      "weekday : 0.029771706492512037\n",
      "weekend : 0.01539462099643287\n",
      "hour : 0.05166155392811931\n",
      "day_period : 0.022634575436653575\n",
      "night : 0.01596522430855064\n",
      "duration : nan\n",
      "average_time : nan\n",
      "distinct_nb : 0.046137172607572434\n",
      "repetitive_nb : 0.03641206907862046\n",
      "same_category : 0.08184998398318331\n",
      "diff_category : 0.0704978594689984\n",
      "last_item : 0.4308699309693907\n",
      "most_viewed_item : 0.4957581805865106\n",
      "length : 0.05058107704663821\n",
      "longest_item : 0.4317797983282462\n",
      "categories_nb : nan\n",
      "hash_items : 0.0001317630332520854\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "for fe in features_name:\n",
    "    if fe not in (\"lol\"):\n",
    "        data = df[[fe, \"item_id\"]].to_numpy()\n",
    "\n",
    "        #Chi-squared test statistic, sample size, and minimum of rows and columns\n",
    "        X2 = stats.chi2_contingency(data, correction=False)[0]\n",
    "        n = np.sum(data)\n",
    "        minDim = min(data.shape)-1\n",
    "\n",
    "        #calculate Cramer's V \n",
    "        V = np.sqrt((X2/n) / minDim)\n",
    "\n",
    "        #display Cramer's V\n",
    "        print(fe, \":\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdi = df.select(\"stddev ('item_id')\")\n",
    "print(sdi)\n",
    "for feature in features_name:\n",
    "    print(f\"{feature} : {df.cov('item_id', feature)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Forward feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 3 : Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = BIG_RDD.map(lambda x: LabeledPoint(x[1][0], list(x[1][1:-4])+[str(x[1][4])]+list(x[1][-3:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model = NaiveBayes.train(test)\n",
    "#nb = NaiveBayes()\n",
    "#nb.setFeaturesCol(\"month\")\n",
    "#model = nb.fit(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/mllib-naive-bayes.html\n",
    "\n",
    "predictionAndLabel = BIG_RDD.map(lambda x: (model.predict(list(x[1][1:-4])+[str(x[1][4])]+list(x[1][-3:])), x[1][0]))\n",
    "accuracy = 1.0 * predictionAndLabel.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "print('model accuracy {}'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
