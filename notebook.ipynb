{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# INTRODUCTION\n",
    "In this notebook we will be using the following packages as bullet list:\n",
    "  * [pyspark](https://spark.apache.org/docs/latest/api/python/pyspark.html)\n",
    "  * [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
    "  * [pyarrow](https://arrow.apache.org/docs/python/index.html)\n",
    "\n",
    "We will be using the following data:\n",
    "  * dressipi_recsys2022\n",
    "\n",
    "The dataset is split in 4 csv files :\n",
    "   * candidate_items : contains the candidate items for each user\n",
    "   * item_features : contains the features for each item (items can have multiple features)\n",
    "   * train_purchases : contains the purchases for each user\n",
    "   * train_sessions : contains the sessions for each user\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Uncomment to install the required packages\n",
    "# %pip install pyspark\n",
    "# %pip install pyarrow\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# from dataset import *\n",
    "# from pipeline import *\n",
    "# from our_model import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "\n",
    "# start spark session\n",
    "spark = SparkSession.builder.appName(\"Python Spark SQL basic example\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "\n",
    "candidate_items, item_features, train_purchases, train_sessions = [None] * 4\n",
    "datasets = [candidate_items, item_features, train_purchases, train_sessions]\n",
    "\n",
    "#load all datasets\n",
    "def load_candidate_items():\n",
    "    global candidate_items\n",
    "    candidate_items = spark.read.csv(\"dressipi_recsys2022/candidate_items.csv\", header=True)\n",
    "    candidate_items = candidate_items.withColumn(\"item_id\", candidate_items[\"item_id\"].cast(\"int\"))\n",
    "\n",
    "def load_item_features():\n",
    "    global item_features\n",
    "    item_features = spark.read.csv(\"dressipi_recsys2022/item_features.csv\", header=True)\n",
    "    item_features = item_features.withColumn(\"item_id\", item_features[\"item_id\"].cast(\"int\"))\n",
    "    item_features = item_features.withColumn(\"feature_category_id\", item_features[\"feature_category_id\"].cast(\"int\"))\n",
    "    item_features = item_features.withColumn(\"feature_value_id\", item_features[\"feature_value_id\"].cast(\"int\"))\n",
    "\n",
    "def load_train_purchases():\n",
    "    global train_purchases\n",
    "    train_purchases = spark.read.csv(\"dressipi_recsys2022/train_purchases.csv\", header=True)\n",
    "    train_purchases = train_purchases.withColumn(\"session_id\", train_purchases[\"session_id\"].cast(\"int\"))\n",
    "    train_purchases = train_purchases.withColumn(\"item_id\", train_purchases[\"item_id\"].cast(\"int\"))\n",
    "    train_purchases = train_purchases.withColumn(\"date\", train_purchases[\"date\"].cast(\"timestamp\"))\n",
    "\n",
    "def load_train_sessions():\n",
    "    global train_sessions\n",
    "    train_sessions = spark.read.csv(\"dressipi_recsys2022/train_sessions.csv\", header=True)\n",
    "    train_sessions = train_sessions.withColumn(\"session_id\", train_sessions[\"session_id\"].cast(\"int\"))\n",
    "    train_sessions = train_sessions.withColumn(\"user_id\", train_sessions[\"item_id\"].cast(\"int\"))\n",
    "    train_sessions = train_sessions.withColumn(\"date\", train_sessions[\"date\"].cast(\"timestamp\"))\n",
    "\n",
    "def load_datasets():\n",
    "    global datasets\n",
    "    load_candidate_items()\n",
    "    load_item_features()\n",
    "    load_train_purchases()\n",
    "    load_train_sessions()\n",
    "    datasets = [candidate_items, item_features, train_purchases, train_sessions]\n",
    "\n",
    "load_datasets()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quick look at the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### train_sessions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: integer (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      "\n",
      "+----------+-------+--------------------+-------+\n",
      "|session_id|item_id|                date|user_id|\n",
      "+----------+-------+--------------------+-------+\n",
      "|         3|   9655|2020-12-18 21:25:...|   9655|\n",
      "|         3|   9655|2020-12-18 21:19:...|   9655|\n",
      "|        13|  15654|2020-03-13 19:35:...|  15654|\n",
      "|        18|  18316|2020-08-26 19:18:...|  18316|\n",
      "|        18|   2507|2020-08-26 19:16:...|   2507|\n",
      "+----------+-------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_sessions.printSchema()\n",
    "train_sessions.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### train_purchases"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+\n",
      "|session_id|item_id|                date|\n",
      "+----------+-------+--------------------+\n",
      "|         3|  15085|2020-12-18 21:26:...|\n",
      "|        13|  18626|2020-03-13 19:36:...|\n",
      "|        18|  24911|2020-08-26 19:20:...|\n",
      "|        19|  12534|2020-11-02 17:16:...|\n",
      "|        24|  13226|2020-02-26 18:27:...|\n",
      "+----------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_purchases.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### item_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+\n",
      "|item_id|feature_category_id|feature_value_id|\n",
      "+-------+-------------------+----------------+\n",
      "|      2|                 56|             365|\n",
      "|      2|                 62|             801|\n",
      "|      2|                 68|             351|\n",
      "|      2|                 33|             802|\n",
      "|      2|                 72|              75|\n",
      "+-------+-------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_features.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### candidate_items"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|item_id|\n",
      "+-------+\n",
      "|      4|\n",
      "|      8|\n",
      "|      9|\n",
      "|     19|\n",
      "|     20|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "candidate_items.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1 : Pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checking for null values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate_items: 0\n",
      "item_features: 0\n",
      "train_purchases: 0\n",
      "train_sessions: 0\n"
     ]
    }
   ],
   "source": [
    "# Are there missing values (na or null) in any of those dataframes ? print the number of missing values\n",
    "print(\"candidate_items:\", candidate_items.filter(candidate_items[\"item_id\"].isNull()).count())\n",
    "print(\"item_features:\", item_features.filter(item_features[\"item_id\"].isNull()).count())\n",
    "print(\"train_purchases:\", train_purchases.filter(train_purchases[\"item_id\"].isNull()).count())\n",
    "print(\"train_sessions:\", train_sessions.filter(train_sessions[\"item_id\"].isNull()).count())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature engineering"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Session duration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[session_id: int, item_id: string, date: timestamp]"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sessions = train_sessions.drop(\"user_id\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(48, datetime.timedelta(seconds=692, microseconds=820000)), (208, datetime.timedelta(seconds=20, microseconds=598000)), (352, datetime.timedelta(seconds=549, microseconds=149000)), (384, datetime.timedelta(seconds=436, microseconds=156000)), (464, datetime.timedelta(seconds=90, microseconds=695000))]\n",
      "[(48, 3), (208, 2), (352, 3), (384, 11), (464, 2)]\n"
     ]
    }
   ],
   "source": [
    "# a = train_sessions.rdd.map(lambda x: x.session_id)\n",
    "# a.take(5)\n",
    "\n",
    "def get_session_duration(times):\n",
    "    times.sort()\n",
    "    return times[-1] - times[0]\n",
    "\n",
    "# groupedBySessionID = train_sessions.rdd.union(train_purchases.rdd).groupBy(lambda x: x.session_id)\n",
    "\n",
    "\n",
    "\n",
    "# sessions_duration = groupedBySessionID.map(lambda y: (y[0], get_session_duration([z.date for z in y[1]])))\n",
    "\n",
    "# sessions_length = groupedBySessionID.map(lambda y: (y[0], len(list(y[1]))))\n",
    "\n",
    "# print(sessions_duration.take(5))\n",
    "# print(sessions_length.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "fullData = train_sessions.groupBy(\"session_id\").agg(F.collect_list(\"item_id\"), F.collect_list(\"date\")).join(train_purchases, \"session_id\", \"fullouter\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Purchase rate of every item"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "purchase_rate = train_purchases.groupBy(\"item_id\").agg(F.count(\"session_id\"))\n",
    "purchase_sum = purchase_rate.agg(F.sum(\"count(session_id)\")).collect()[0][0]\n",
    "print(purchase_sum)\n",
    "purchase_rate = purchase_rate.withColumn(\"rate\", purchase_rate[\"count(session_id)\"] / purchase_sum)\n",
    "purchase_rate = purchase_rate.drop(\"count(session_id)\")\n",
    "\n",
    "# train_purchases.select(F.countDistinct(\"item_id\")).show(5)\n",
    "# candidate_items.select(F.countDistinct(\"item_id\")).show(5)\n",
    "# train_purchases.selectExpr(\"count(distinct(item_id))\").show(5)\n",
    "# candidate_items.selectExpr(\"count(distinct(item_id))\").show(5)\n",
    "\n",
    "# item_features.select(F.countDistinct(\"item_id\")).show(5)\n",
    "# purchase_rate = purchase_rate.join(candidate_items, \"item_id\", \"right\")\n",
    "\n",
    "# purchase_rate = purchase_rate.fillna(0, \"rate\")\n",
    "# purchase_rate.show(5)\n",
    "# purchase_rate.sort(F.asc(\"rate\")).show(5)\n",
    "#purchase_rate.where(\"rate==null\").show(5)\n",
    "# purchase_rate.agg(F.sum(\"rate\")).show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can transform our data to an RDD."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m rdd \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msparkContext\u001B[38;5;241m.\u001B[39mparallelize(\u001B[43mdata\u001B[49m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2 : Algorithms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scalable feature selection algorithm 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scalable feature selection algorithm 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ranking algorithm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Forward feature selection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 3 : Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show rows with item_id == 2 in all dataframes\n",
    "for d in datasets:\n",
    "    print(d.filter(d[\"item_id\"] == 2).collect())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}