{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# <center>INFO-H515 - Distributed Data Management and Scalable Analytics</center>\n",
    "\n",
    "## <center>Project 2021-2022</center>\n",
    "\n",
    "#### <center>Bakkali Yahya (000445166)</center>\n",
    "#### <center>Hauwaert Maxime (000461714)</center>\n",
    "#### <center>Marotte Nathan (000459274)</center>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# INTRODUCTION\n",
    "\n",
    "\n",
    "In this notebook we will be using the following packages:\n",
    "  * [PySpark](https://spark.apache.org/docs/latest/api/python/pyspark.html)\n",
    "  * [Pandas](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
    "  * [Numpy](https://numpy.org/doc/stable/)\n",
    "\n",
    "## Specifications of the problem\n",
    "We are tasked to construct a recommender system as part of the recsyschallenge of 2022, organised by Dressipi, a company focused on providing product and outfit recommendations to leading global retailers.\n",
    "\n",
    "## Dataset\n",
    "The full dataset consists of 1.1 million online retail sessions in the fashion domain, sampled from a 18-month period.\n",
    "\n",
    "It is split in 4 csv files:\n",
    "   * candidate_items.csv : contains the candidate items for the recommender system. This means our model will provide those item_id as output of the prediction.\n",
    "   * item_features.csv : contains the features (for example material, type, colour, etc ... ) for each item, as well as the value of this feature (cotton, skinny, blue, etc ...)\n",
    "   * train_purchases.csv : contains the purchases at the end of each session.\n",
    "   * train_sessions.csv : contains the purchasing sessions, each session is a stream of items viewed, and when an item is purchased, the session ends.\n",
    "\n",
    "Using those 4 files, we will first develop a pipeline to trim it, then engineer the features to regroup different features into one dataset so that it can be used for the recommender system.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating the Spark session"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# start spark session\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=32g pyspark-shell\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"AppName\") \\\n",
    "    .config('spark.ui.port', '4075')\\\n",
    "    .getOrCreate()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "candidate_items, item_features, train_purchases, train_sessions = [None] * 4\n",
    "datasets = [candidate_items, item_features, train_purchases, train_sessions]\n",
    "\n",
    "#load all datasets\n",
    "def load_candidate_items():\n",
    "    global candidate_items\n",
    "    candidate_items = spark.read.csv(\"dressipi_recsys2022/candidate_items.csv\", header=True)\n",
    "    candidate_items = candidate_items.withColumn(\"item_id\", candidate_items[\"item_id\"].cast(\"int\"))\n",
    "\n",
    "def load_item_features():\n",
    "    global item_features\n",
    "    item_features = spark.read.csv(\"dressipi_recsys2022/item_features.csv\", header=True)\n",
    "    item_features = item_features.withColumn(\"item_id\", item_features[\"item_id\"].cast(\"int\"))\n",
    "    item_features = item_features.withColumn(\"feature_category_id\", item_features[\"feature_category_id\"].cast(\"int\"))\n",
    "    item_features = item_features.withColumn(\"feature_value_id\", item_features[\"feature_value_id\"].cast(\"int\"))\n",
    "\n",
    "def load_train_purchases():\n",
    "    global train_purchases\n",
    "    train_purchases = spark.read.csv(\"dressipi_recsys2022/train_purchases.csv\", header=True)\n",
    "    train_purchases = train_purchases.withColumn(\"session_id\", train_purchases[\"session_id\"].cast(\"int\"))\n",
    "    train_purchases = train_purchases.withColumn(\"item_id\", train_purchases[\"item_id\"].cast(\"int\"))\n",
    "    train_purchases = train_purchases.withColumn(\"date\", train_purchases[\"date\"].cast(\"timestamp\"))\n",
    "\n",
    "def load_train_sessions():\n",
    "    global train_sessions\n",
    "    train_sessions = spark.read.csv(\"dressipi_recsys2022/train_sessions.csv\", header=True)\n",
    "    train_sessions = train_sessions.withColumn(\"session_id\", train_sessions[\"session_id\"].cast(\"int\"))\n",
    "    train_sessions = train_sessions.withColumn(\"item_id\", train_sessions[\"item_id\"].cast(\"int\"))\n",
    "    train_sessions = train_sessions.withColumn(\"date\", train_sessions[\"date\"].cast(\"timestamp\"))\n",
    "\n",
    "def load_datasets():\n",
    "    global datasets\n",
    "    load_candidate_items()\n",
    "    load_item_features()\n",
    "    load_train_purchases()\n",
    "    load_train_sessions()\n",
    "    datasets = [candidate_items, item_features, train_purchases, train_sessions]\n",
    "\n",
    "load_datasets()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Quick look at the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### train_sessions.csv\n",
    "This dataset represents the browsing session of a user in the store. It is made of 3 columns:\n",
    "- session_id : the id of the session. It serves as a key to join the data with the other datasets\n",
    "- item_id : the item viewed during the session.\n",
    "- date : the date of at wich the item was viewed.\n",
    "\n",
    "Please note that the browsing sessions end at the end of the day, or if an item was purchased. This means we will not find, for one session, items viewed on 2 different days.\n",
    "Also, as stated on the challenge's website, there are no sessions that do not end with a purchased item in this dataset.\n",
    "\n",
    "Here is a representation of a few of the rows in the train_sessions.csv dataset:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_sessions.show(5)\n",
    "print(f\"There are {train_sessions.count()} rows in total.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### train_purchases.csv\n",
    "\n",
    "This dataset represents the purchases made by a user in the store. It is made of 3 columns:\n",
    "- session_id : the id of the session.\n",
    "- item_id : the item purchased.\n",
    "- date : the date of at wich the item was purchased.\n",
    "\n",
    "This dataset should be used with the train_sessions dataset to have a more complete picture of the browsing experience of the user. Each session in this dataset ends with an item purchased, noted in the column item_id.\n",
    "\n",
    "Here is a representation of a few of the rows in the train_purchases.csv dataset:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_purchases.show(5)\n",
    "print(f\"There are {train_purchases.count()} rows in total.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### item_features.csv\n",
    "\n",
    "This dataset helps us notice pattern in different objects. It links the item_id with their feature (such as the color of the item, the material, the type, etc ...).\n",
    "There are 3 columns :\n",
    "- item_id : the id of the item in the store.\n",
    "- feature_id : the id of the feature attached to the item.\n",
    "- value : the value of the feature for that item. (for example if the feature_category_id is the color, the feature_value_id could be a representation of \"red\", \"blue\", \"green\", etc ...)\n",
    "\n",
    "Here is a representation of a few of the rows in the item_features.csv dataset:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "item_features.show(5)\n",
    "print(f\"There are {item_features.count()} rows in total.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### candidate_items.csv\n",
    "\n",
    "This dataset contains all the item_id that are candidate to the recommandation system. It consists of a column of 4991 item_id.\n",
    "\n",
    "Here is a representation of a few of the rows in the candidate_items.csv dataset:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "candidate_items.show(5)\n",
    "print(f\"There are {candidate_items.count()} rows in total.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1 : Pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data exploration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will first check if there are missing values, NA, or Null values in the downloaded dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Nb of missing values in candidate_items:\", candidate_items.filter(candidate_items[\"item_id\"].isNull()).count())\n",
    "print(\"Nb of missing values in item_features:\", item_features.filter(item_features[\"item_id\"].isNull()).count())\n",
    "print(\"Nb of missing values in train_purchases:\", train_purchases.filter(train_purchases[\"item_id\"].isNull()).count())\n",
    "print(\"Nb of missing values in train_sessions:\", train_sessions.filter(train_sessions[\"item_id\"].isNull()).count())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we wondered if there are sessions for which the first item viewed was purchased (therefore it will not be in train_sessions but in train_purchases), and we also made sure that there are no sessions for which there were no purchases."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = [a[\"session_id\"] for a in train_sessions.select(\"session_id\").collect()]\n",
    "p = [a[\"session_id\"] for a in train_purchases.select(\"session_id\").collect()]\n",
    "\n",
    "print(f\"Nb of session id in train_sessions but not in train_purchases: {len(set(t).difference(set(p)))}\")\n",
    "print(f\"Nb of session id in train_purchases but not in train_sessions: {len(set(p).difference(set(t)))}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we wanted to know how much data we were working with so we counted the number of different item_id in each dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = train_sessions.select(\"item_id\").distinct().collect()\n",
    "b = train_purchases.select(\"item_id\").distinct().collect()\n",
    "c = candidate_items.select(\"item_id\").distinct().collect()\n",
    "d = item_features.select(\"item_id\").distinct().collect()\n",
    "print(f\"Nb of distinct item id in train_sessions: {len(a)}\")\n",
    "print(f\"Nb of distinct item id in train_purchases: {len(b)}\")\n",
    "print(f\"Nb of distinct item id in candidate_items: {len(c)}\")\n",
    "print(f\"Nb of distinct item id in item_features: {len(d)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We cannot know if those items are the same or different, but by running the next cell we discover that there are no items in train_sessions that are not in another dataset. Therefore we know that we have at least 23496 items."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Nb of item id in train_sessions but not in item_features: {len(set(a).difference(set(d)))}\")\n",
    "print(f\"Nb of item id in train_sessions but not in train_purchases: {len(set(b).difference(set(d)))}\")\n",
    "print(f\"Nb of item id in train_sessions but not in candidate_items: {len(set(c).difference(set(d)))}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature engineering"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's remove sessions which end up with the purchase of a non candidate item."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_purchases = candidate_items.join(train_purchases, \"item_id\", \"inner\")\n",
    "\n",
    "train_sessions = train_sessions.join(train_purchases.select(\"session_id\"), \"session_id\", \"inner\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's transform the dataset from a dataframe into an RDD."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Maps every session to its items and dates\n",
    "session_item_date_rdd = train_sessions.rdd.map(lambda x: (x[\"session_id\"], (x[\"item_id\"], x[\"date\"]))).cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These lists will contain the RDDs of the features along with their names."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = []\n",
    "features_names = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Date related features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's create an RDD that will be used to generate the following features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Maps every session to its dates\n",
    "session_date_rdd = session_item_date_rdd.mapValues(lambda x: (x[1])).cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1) Month\n",
    "\n",
    "The month of the year (1 to 12) in which the session took place\n",
    "\n",
    "The reason for this feature is that the pieces of fashion are probably more likely to be bought depending on the month of the purchase. There is probably less swimsuits bought in December than in July, therefore we believe that if two items are bought in the same month, they could be similar."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_month_feature():\n",
    "    return session_date_rdd.mapValues(lambda x: int(x.month))\\\n",
    "                            .reduceByKey(min)\n",
    "    \n",
    "month = get_month_feature()\n",
    "\n",
    "features.append(month)\n",
    "features_names.append(\"month\")\n",
    "\n",
    "print(month.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2) Season (Meteorological)\n",
    "\n",
    "The meteorological season of the year in which the session takes place.\n",
    "  - Winter : December, January, and February\n",
    "  - Spring : March, April, and May\n",
    "  - Summer : June, July, and Augustus\n",
    "  - Fall   : September, October, and November\n",
    "\n",
    "We add this feature even though it seems redudant with the months feature, because the month feature might be too restrictive (stuff bought in january or february can also be similar because of the season, especially in clothing).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_season_feature():\n",
    "    return month.mapValues(get_season)\\\n",
    "                .reduceByKey(min)\n",
    "\n",
    "def get_season(month):\n",
    "    if month == 12 or month <= 2: return 0\n",
    "    elif 2 < month <= 5: return 1\n",
    "    elif 5 < month <= 8: return 2\n",
    "    elif 8 < month <= 11: return 3\n",
    "\n",
    "season = get_season_feature()\n",
    "\n",
    "features.append(season)\n",
    "features_names.append(\"season\")\n",
    "\n",
    "print(season.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3) Day of month\n",
    "\n",
    "We believe that the day of the month (from 1 (#todo 0 ?) to 28, 29, 30 or 31 also has an influence on the purchases, because salaries are often paid close by the 1st of the month, therefore we might expect more purchase during that time.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_day_of_month_feature():\n",
    "    return session_date_rdd.mapValues(lambda x: int(x.day))\\\n",
    "                            .reduceByKey(min)\n",
    "\n",
    "day_of_month = get_day_of_month_feature()\n",
    "\n",
    "features.append(day_of_month)\n",
    "features_names.append(\"day_of_month\")\n",
    "\n",
    "print(day_of_month.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4) Weekday\n",
    "\n",
    "This feature indicates what day of the week (0 to 6, starting on monday) the session happened as we believe this may have an influence, because there is more time to browse on the weekend than on week days where people are probably working."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_weekday_feature():\n",
    "    return session_date_rdd.mapValues(lambda x: int(x.strftime(\"%w\")))\\\n",
    "                            .reduceByKey(min)\n",
    "\n",
    "weekday = get_weekday_feature()\n",
    "\n",
    "features.append(weekday)\n",
    "features_names.append(\"weekday\")\n",
    "\n",
    "print(weekday.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5) Weekend\n",
    "\n",
    "A binary feature to tells us if the session happened on a weekend.\n",
    "    \n",
    "    Maybe the weekday feature is too specific and the ?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_weekend_feature():\n",
    "    return weekday.mapValues(lambda x: int(x in (5, 6)))\n",
    "\n",
    "weekend = get_weekend_feature()\n",
    "\n",
    "features.append(weekend)\n",
    "features_names.append(\"weekend\")\n",
    "\n",
    "print(weekend.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 6) Hour\n",
    "\n",
    "This feature give us the 24-hour format hour of the session.\n",
    "\n",
    "The justification is that there are probably specific items bought late in the night, or other items bought a few hours after the end of workday.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_hour_feature():\n",
    "    return session_date_rdd.mapValues(lambda x: int(x.strftime(\"%H\")))\\\n",
    "                            .reduceByKey(min)\n",
    "\n",
    "hour = get_hour_feature()\n",
    "\n",
    "features.append(hour)\n",
    "features_names.append(\"hour\")\n",
    "\n",
    "print(hour.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7) Day period\n",
    "#todo reprendre ici\n",
    "\n",
    "    It tells us at which period of the day the session began.\n",
    "    \n",
    "    Maybe the hour feature is too restrictive ?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_day_period_feature():\n",
    "    return hour.mapValues(get_day)\n",
    "\n",
    "def get_day(x):\n",
    "    if 6 < x < 12: return 0\n",
    "    elif 12 < x < 18: return 1\n",
    "    elif 18 < x < 22: return 2\n",
    "    else: return 3\n",
    "    \n",
    "day_period = get_day_period_feature()\n",
    "\n",
    "features.append(day_period)\n",
    "features_names.append(\"day_period\")\n",
    "\n",
    "print(day_period.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 8) Night\n",
    "\n",
    "    It tells us if the session began at night.\n",
    "    \n",
    "    Maybe the day period feature is still too restrictive ?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_night_feature():\n",
    "    return day_period.mapValues(lambda x: int(x == 3))\n",
    "\n",
    "night = get_night_feature()\n",
    "\n",
    "features.append(night)\n",
    "features_names.append(\"night\")\n",
    "\n",
    "print(night.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 9) Duration of the session\n",
    "\n",
    "    It tells us the duration of the session.\n",
    "    \n",
    "    ?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_duration_feature():\n",
    "    return session_date_rdd.groupByKey()\\\n",
    "                            .mapValues(get_session_duration)\n",
    "\n",
    "def get_session_duration(dates):\n",
    "    dates = list(dates)\n",
    "    dates.sort()\n",
    "    return (dates[-1] - dates[0]).total_seconds() if len(dates) >= 2 else 0.0\n",
    "\n",
    "duration = get_duration_feature()\n",
    "\n",
    "features.append(duration)\n",
    "features_names.append(\"duration\")\n",
    "\n",
    "print(duration.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "10) Average time between consecutive item views\n",
    "\n",
    "    It tells us the average of time between the consecutive views of the items of the session.\n",
    "    \n",
    "    ?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_average_time_feature():\n",
    "    return session_date_rdd.groupByKey()\\\n",
    "                            .mapValues(get_average_time)\n",
    "\n",
    "def get_average_time(dates):\n",
    "    import datetime\n",
    "    dates = sorted(list(dates))\n",
    "    avgs = [dates[i+1] - dates[i] for i in range(len(dates)-1)]\n",
    "    return round((sum(avgs, datetime.timedelta())/len(avgs)).total_seconds()) if len(avgs) > 0 else 0\n",
    "\n",
    "average_time = get_average_time_feature()\n",
    "\n",
    "features.append(average_time)\n",
    "features_names.append(\"average_time\")\n",
    "\n",
    "print(average_time.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Session related features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's create an RDD that will be used to generate the following features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Maps every session to its items\n",
    "session_item_rdd = session_item_date_rdd.mapValues(lambda x: (x[0])).cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 11) Number of items\n",
    "\n",
    "    It tells us the number of items seen in the session.\n",
    "    \n",
    "    We believe that alone it is useless but by combining it with the following features, it could become useful."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_length_feature():\n",
    "    return session_item_rdd.groupByKey().mapValues(len)\n",
    "\n",
    "length = get_length_feature()\n",
    "\n",
    "features.append(length)\n",
    "features_names.append(\"length\")\n",
    "\n",
    "print(length.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 12) Number of distinct items\n",
    "\n",
    "    It tells us the number of distinct items viewed in the session.\n",
    "    \n",
    "    We believe that this feature could add a dimension to the feature \"number of items\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_distinct_nb_feature():\n",
    "    return session_item_rdd.groupByKey().mapValues(lambda x: len(set(x)))\n",
    "\n",
    "distinct_nb = get_distinct_nb_feature()\n",
    "\n",
    "features.append(distinct_nb)\n",
    "features_names.append(\"distinct_nb\")\n",
    "\n",
    "print(distinct_nb.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Items related features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 13) Most viewed item\n",
    "\n",
    "    It tells us the most seen item in the session.\n",
    "    \n",
    "    The most seen item should be highly be related to the purchased item."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_most_viewed_item_feature():\n",
    "    return session_item_rdd.groupByKey().mapValues(get_most_viewed_item)\n",
    "\n",
    "def get_most_viewed_item(items):\n",
    "    items = list(items)\n",
    "    items.sort()\n",
    "    most_viewed = (None, -1)\n",
    "    \n",
    "    last_viewed = items[0]\n",
    "    cnt = 0\n",
    "    \n",
    "    for item in items:\n",
    "        if last_viewed != item:\n",
    "            if cnt > most_viewed[1]:\n",
    "                most_viewed = (last_viewed, cnt)\n",
    "                cnt = 1\n",
    "                last_viewed = item\n",
    "        else:\n",
    "            cnt += 1\n",
    "            \n",
    "    if cnt > most_viewed[1]:\n",
    "        most_viewed = (last_viewed, cnt)\n",
    "    \n",
    "    return most_viewed[0]\n",
    "    \n",
    "most_viewed_item = get_most_viewed_item_feature()\n",
    "\n",
    "features.append(most_viewed_item)\n",
    "features_names.append(\"most_viewed_item\")\n",
    "\n",
    "print(most_viewed_item.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 14) Longest viewed item\n",
    "\n",
    "    It tells us the item that was seen the longest in the session.\n",
    "    \n",
    "    The longest viewed item should be highly related to the purchased item as it tells us that the user was most likely interested in it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_longest_item_feature():\n",
    "    return train_sessions.rdd.map(lambda x: (x[\"session_id\"], (x[\"item_id\"], x[\"date\"]))).\\\n",
    "                    groupByKey().\\\n",
    "                    mapValues(get_longest_item)\n",
    "\n",
    "def get_longest_item(items):\n",
    "    items = list(items)\n",
    "    t = [items[i+1][1] - items[i][1] for i in range(len(items)-1)]\n",
    "    return items[np.argmax(t)][0] if len(t) > 0 else items[0][0]\n",
    "\n",
    "longest_item = get_longest_item_feature()\n",
    "\n",
    "features.append(longest_item)\n",
    "features_names.append(\"longest_item\")\n",
    "\n",
    "print(longest_item.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 15) Last item\n",
    "\n",
    "    It tells us the last item that was seen in the session.\n",
    "    \n",
    "    The last item seen in the session should be closer to the purchased item than the first one."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_last_item_feature():\n",
    "    return session_item_date_rdd.groupByKey()\\\n",
    "                                .mapValues(lambda x: max(x, key=lambda i: i[1])[0])\n",
    "\n",
    "def get_last_item(items):\n",
    "    return max(list(items), key=lambda i: i[1])[0]\n",
    "\n",
    "last_item = get_last_item_feature()\n",
    "\n",
    "features.append(last_item)\n",
    "features_names.append(\"last_item\")\n",
    "\n",
    "print(last_item.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Items' categories related features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's create two RDDs that will be used to generate the following features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Maps every item to its features\n",
    "item_features_rdd = item_features.rdd.map(lambda x: (x[\"item_id\"], (x[\"feature_category_id\"], x[\"feature_value_id\"])))\\\n",
    "                                .groupByKey()\\\n",
    "                                .mapValues(lambda x: [(a,b) for a, b in x])\n",
    "\n",
    "# Maps every session to the features of its items\n",
    "session_item_features_rdd = item_features_rdd.join(train_sessions.rdd.map(lambda x: (x[\"item_id\"], x[\"session_id\"])))\\\n",
    "                                            .map(lambda x: (x[1][1], x[1][0]))\\\n",
    "                                            .groupByKey()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 16) Most present number of categories\n",
    "    # TODO rename\n",
    "    It tells us the most present number of categories of the items seen in the session.\n",
    "    \n",
    "    The number of categories of an item should be closely related to its type.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_categories_nb_feature():\n",
    "    return session_item_features_rdd.mapValues(get_categories_nb)\n",
    "\n",
    "def get_categories_nb(cat):\n",
    "    lst = [len(c) for c in cat]\n",
    "    return max(set(lst), key=lst.count)\n",
    "    \n",
    "categories_nb = get_categories_nb_feature()\n",
    "\n",
    "features.append(categories_nb)\n",
    "features_names.append(\"categories_nb\")\n",
    "\n",
    "print(categories_nb.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 17 - 18 ) Most present category with its count\n",
    "\n",
    "    It tells us the most present category in the different items seen in the session and the number of times it appears.\n",
    "    \n",
    "    The most present category should be highly be related to the purchased item."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We generate for each session the most present category and the number of times it appears."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_most_present_category_with_count(categories_i):\n",
    "    categories = [cat for cat_i in categories_i for cat in cat_i]    \n",
    "    categories.sort()\n",
    "    \n",
    "    most_viewed = (None, -1)\n",
    "    last_viewed = categories[0]\n",
    "    cnt = 0\n",
    "    \n",
    "    for category in categories:\n",
    "        if last_viewed != category:\n",
    "            if cnt > most_viewed[1]:\n",
    "                most_viewed = (last_viewed, cnt)\n",
    "                cnt = 1\n",
    "                last_viewed = category\n",
    "        else:\n",
    "            cnt += 1\n",
    "            \n",
    "    if cnt > most_viewed[1]:\n",
    "        most_viewed = (last_viewed, cnt)\n",
    "    \n",
    "    return most_viewed[0], most_viewed[1]\n",
    "\n",
    "most_present_category_with_count = session_item_features_rdd.mapValues(get_most_present_category_with_count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We split the RDD above in two."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Models in pyspark require the features to have a numeric value. So to solve easily this issue we created a simple function that return the sum of ($category$ * 1000) and the $value$. We chose 1000 as the maximum value that $value$ can take is less than 1000, this ensures no collision.\n",
    "\n",
    "For example: (category, value) : (66, 109) -> 66 000 + 109 = 66 109"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_most_present_category_feature():\n",
    "    return most_present_category_with_count.mapValues(lambda x: x[0][0]*1000 + x[0][1])\n",
    "\n",
    "most_present_category = get_most_present_category_feature()\n",
    "\n",
    "features.append(most_present_category)\n",
    "features_names.append(\"most_present_category\")\n",
    "\n",
    "print(most_present_category.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_most_present_category_count_feature():\n",
    "    return most_present_category_with_count.mapValues(lambda x: x[1])\n",
    "\n",
    "\n",
    "most_present_category_count = get_most_present_category_count_feature()\n",
    "\n",
    "features.append(most_present_category_count)\n",
    "features_names.append(\"most_present_category_count\")\n",
    "\n",
    "print(most_present_category_count.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "12) Number of repetitive items\n",
    "\n",
    "    It tells us ?\n",
    "    \n",
    "    ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"def get_repetitive_nb_feature():\n",
    "    return session_item_rdd.groupByKey().mapValues(lambda x: len(list(x)) - len(set(x)))\n",
    "\n",
    "repetitive_nb = get_repetitive_nb_feature()\n",
    "print(repetitive_nb.take(5))\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "13) Same category\n",
    "\n",
    "\n",
    "    It tells us the number of categories which appear at least twice in the items of the session.\n",
    "    \n",
    "    ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"def get_same_category_feature():\n",
    "    return session_item_features_rdd.mapValues(get_same_category)\n",
    "\n",
    "def get_same_category(x):\n",
    "    dico = dict()\n",
    "    for item in x:\n",
    "        for cat in item:\n",
    "            if cat in dico:\n",
    "                dico[cat] += 1\n",
    "            else:\n",
    "                dico[cat] = 0\n",
    "\n",
    "    res = 0\n",
    "    for val in dico.values():\n",
    "        if val > 0:\n",
    "            res += 1\n",
    "\n",
    "    return res\n",
    "\n",
    "same_category = get_same_category_feature()\n",
    "print(same_category.take(5))\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "14) Different category\n",
    "\n",
    "    It tells us ?\n",
    "    \n",
    "    ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"def get_diff_category():\n",
    "    return session_item_features_rdd.mapValues(get_different_category)\n",
    "\n",
    "def get_different_category(x):\n",
    "    dico = dict()\n",
    "    for item in x:\n",
    "        for cat in item:\n",
    "            if cat in dico:\n",
    "                dico[cat] += 1\n",
    "            else:\n",
    "                dico[cat] = 0\n",
    "\n",
    "    res = 0\n",
    "    for val in dico.values():\n",
    "        if val == 0:\n",
    "            res += 1\n",
    "\n",
    "    return res\n",
    "\n",
    "diff_category = get_diff_category()\n",
    "print(diff_category.take(5))\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Joining features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All of these engineered features will be put in the same RDD with the id of the session as the key and the different features as the value.\n",
    "\n",
    "This will be done through consecutive join's on the key."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "purchase_item_rdd = train_purchases.rdd.map(lambda x: (x[\"session_id\"], x[\"item_id\"])).cache()\n",
    "\n",
    "def get_full_rdd():\n",
    "    temp = purchase_item_rdd.join(features[0])\n",
    "    for i in range(1, len(features)):\n",
    "        feature = features[i]\n",
    "        temp = temp.join(feature).mapValues(lambda x: list(x[0])+[x[1]])\n",
    "    return temp\n",
    "\n",
    "BIG_RDD = get_full_rdd().cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(BIG_RDD.take(1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aha ha # TEST"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2 : Feature selection\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ranking algorithm\n",
    "\n",
    "According to G. Bontempi's handbook \"Statistical foundations of machine learning\", the ranking methods follow these steps:\n",
    "    \n",
    "    1) Calculate for each feature its relevance with the output variable using a univariate measure.\n",
    "    \n",
    "    2) Sort their relevances in descending order.\n",
    "    \n",
    "    3) Select the top k features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we have a lot of categorical features as well as a categorical output, we chose to use mutual information as a univariate measure.\n",
    "\n",
    "$$ I(X;Y) = H(Y) - H(Y|X)$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Entropy-mutual-information-relative-entropy-relation-diagram.svg/1200px-Entropy-mutual-information-relative-entropy-relation-diagram.svg.png"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First let's compute $H(Y)$ in map reduce.\n",
    "\n",
    "$$\\displaystyle \\mathrm {H} (Y)=-\\sum _{y\\in {\\mathcal {Y}}}p(y)\\log _{2}p(x)$$\n",
    "Based on https://en.wikipedia.org/wiki/Entropy_(information_theory)\n",
    "\n",
    "First for each session we produce one element with its label as the key and $1$ as the value.\n",
    "\n",
    "Then reduce by key by adding the values, this gets us the count for each label.\n",
    "\n",
    "After that we map to get $p(y)$ by dividing the count by the total.\n",
    "\n",
    "TODO"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_count = purchase_item_rdd.count()\n",
    "\n",
    "total_count_broadcast = spark.sparkContext.broadcast(total_count)\n",
    "\n",
    "H_y = BIG_RDD.map(lambda x: (x[1][0], 1))\\\n",
    "            .reduceByKey(np.add)\\\n",
    "            .map(lambda x: x[1]/total_count_broadcast.value)\\\n",
    "            .map(lambda x: -x * np.log2(x))\\\n",
    "            .reduce(np.add)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the value of $H(Y)$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "H_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's calculate $H(Y|X)$ of each feature in map reduce.\n",
    "\n",
    "$$\\displaystyle \\mathrm {H} (Y|X)=-\\sum _{y,x\\in {\\mathcal {Y}}\\times {\\mathcal {X}}}p_{Y,X}(y,x)\\log {\\frac {p_{Y,X}(y,x)}{p_{X}(x)}}$$\n",
    "Based on https://en.wikipedia.org/wiki/Entropy_(information_theory)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RDD containing count of each (feature_index, feature_value, label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# (feature_index, feature_value, label) : count\n",
    "count_yx = BIG_RDD.flatMap(lambda x: [((i, x[1][i], x[1][0]), 1) for i in range(1, len(x[1]))])\\\n",
    "                    .reduceByKey(np.add)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RDD containing p of each (feature_index, feature_value, label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# (feature_index, feature_value, label) : p\n",
    "p_yx = count_yx.mapValues(lambda x: x / total_count_broadcast.value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RDD containing count of each (feature_index, feature_value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# (feature_index, feature_value) : count\n",
    "count_x = count_yx.map(lambda x: ((x[0][0], x[0][1]), x[1]))\\\n",
    "                    .reduceByKey(np.add)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RDD containing p of each (feature_index, feature_value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# (feature_index, feature_value) : p\n",
    "p_x = count_x.mapValues(lambda x: x / total_count_broadcast.value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RDD containing each $H(y|x)$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_h_yx_term(x):\n",
    "    p_yx = x[0]\n",
    "    p_y = x[1]\n",
    "    h_yx_term = - p_yx * np.log2(p_yx/p_y)\n",
    "    \n",
    "    return h_yx_term\n",
    "\n",
    "H_yx = p_yx.map(lambda x: ((x[0][0], x[0][1]), x[1]))\\\n",
    "            .join(p_x)\\\n",
    "            .map(lambda x: (x[0][0], get_h_yx_term(x[1])))\\\n",
    "            .reduceByKey(np.add)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "H_yx.take(20) # TODO better print ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After that we can compute $I(X;Y)$ for each feature."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "H_y_broadcast = spark.sparkContext.broadcast(H_y)\n",
    "\n",
    "features_mi = H_xy.mapValues(lambda x: H_y_broadcast.value - x).collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's sort and print the result."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features_mi.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Features ranking:\")\n",
    "\n",
    "for i in features_mi:\n",
    "    print(f\"\\t- {features_names[i[0]-1]} ({i[1]})\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Forward feature selection algorithm\n",
    "\n",
    "According to G. Bontempi's handbook \"Statistical foundations of machine learning\", the forward feature selection algorithms are part of wrapping search strategies and work as follows:\n",
    "    \n",
    "    First, we start with no selected features.\n",
    "    \n",
    "    Then, we select the feature which returns the lowest generalisation error.\n",
    "    \n",
    "    After that, we select the feature which, with the previously selected features, returns the lowest generalisation error.\n",
    "    \n",
    "    The last step is repeated until there is no improvement or the maximum number of features is reached."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's prepare the RDD to train the different models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "all_features_rdd = BIG_RDD.map(lambda x: (x[1][0], x[1][1:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nb_classes = candidate_items.rdd.map(lambda x: x[\"item_id\"]).reduce(max)+1\n",
    "selected_features_indices = []\n",
    "remaining_features_indices = list(range(len(features)))\n",
    "improvement = True\n",
    "last_accuracy = -1\n",
    "\n",
    "while (len(selected_features_indices) < len(features)) and (improvement):\n",
    "    current_accuracy = []\n",
    "    selected_features_indices_broadcast = spark.sparkContext.broadcast(selected_features_indices)\n",
    "    print(\"\\nTEST\")\n",
    "    for current_feature_index in remaining_features_indices:\n",
    "        print(\"-\", end=\"\")\n",
    "        current_feature_index_broadcast = spark.sparkContext.broadcast(current_feature_index)\n",
    "        selected_features_rdd = all_features_rdd.map(lambda x: LabeledPoint(x[0], [x[1][i] for i in selected_features_indices_broadcast.value] + [x[1][current_feature_index_broadcast.value]]))\n",
    "        \n",
    "        training, test = selected_features_rdd.randomSplit([0.6, 0.4], seed = 515)\n",
    "        \n",
    "        model = DecisionTree.trainClassifier(training, nb_classes, {}, maxDepth=2, maxBins=8)\n",
    "        \n",
    "        #predictionAndLabel = test.map(lambda x: (x.label, model.predict(x.features)))\n",
    "        predictionAndLabel = test.map(lambda x: (x.label, 1))\n",
    "        accuracy = 1.0 * predictionAndLabel.filter(lambda x: x[0] == x[1]).count() / test.count()\n",
    "\n",
    "        current_accuracy.append((accuracy, current_feature_index))\n",
    "        \n",
    "    best_feature = max(current_accuracy, key=lambda x: x[0])\n",
    "    \n",
    "    if best_feature[0] > last_accuracy:\n",
    "        last_accuracy = best_feature[0]\n",
    "        best_feature_index = best_feature[1]\n",
    "        selected_features_indices.append(best_feature_index)\n",
    "        remaining_features_indices.remove(best_feature_index)\n",
    "\n",
    "    else:\n",
    "        improvement = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "selected_feature_names = [features_name[i] for i in selected_feature_indices]\n",
    "print(f\"The best features are: \", end=\"\")\n",
    "print(\", \".join(selected_feature_names))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "selected_features_names = features_name # TODO change"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 3 : Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model should be trained on the data with the selected features and should returned the predictions required by the competition. For each test session, the model should return 100 candidates items with the highest chance of being purchased. This restricts the differents models that can be used. Therefore the decision tree model was selected."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's first transform our rdd which contains all the data into a dataframe which the classifier can use."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def collector(x):\n",
    "    dico = {}\n",
    "    dico[\"session_id\"] = x[0]\n",
    "    dico[\"item_id\"] = x[1][0]\n",
    "    \n",
    "    for i in range(len(features_name)):\n",
    "        dico[features_name[i]] = x[1][i+1]\n",
    "        \n",
    "    return dico\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "df = BIG_RDD.map(lambda x: Row(**collector(x))).toDF()\n",
    "df = df.fillna(0)\n",
    "\n",
    "# si = StringIndexer(inputCol=\"item_id\", outputCol=\"label\").fit(item_features.select(\"item_id\").distinct())\n",
    "si = StringIndexer(inputCol=\"item_id\", outputCol=\"label\").fit(candidate_items)\n",
    "indexed_items_df = si.transform(candidate_items)\n",
    "df = si.transform(df)\n",
    "\n",
    "df = VectorAssembler(inputCols = features_name, outputCol = \"features\").transform(df)\n",
    "\n",
    "df = df.select([\"features\", \"label\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we create our model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "decisionTree = DecisionTreeClassifier(maxDepth=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally we can test the accuracy of our model with cross validation. We chose $K$ = 4 to split the dataset into 75 % training and 25 % testing. "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "grid = ParamGridBuilder().addGrid(decisionTree.featuresCol, [\"features\"]).build()\n",
    "cv = CrossValidator(estimator=decisionTree, estimatorParamMaps=grid, numFolds=2, evaluator=evaluator, parallelism=4)\n",
    "cvModel = cv.fit(df)\n",
    "\n",
    "print(\"Average accuracy:\", cvModel.avgMetrics[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the accuracy of our model is quite low. This can be explained by the large number of different items in the dataset. Even in the RecSys Challenge 2022, it asked for each session to give 100 items which have the highest probability."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TEST"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "decisionTree = DecisionTreeClassifier()\n",
    "model = decisionTree.fit(df)\n",
    "\n",
    "pred = model.transform(df)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\") \n",
    "nbaccuracy = evaluator.evaluate(pred) \n",
    "print(\"Test accuracy = \" + str(nbaccuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TEST"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can get the predictions for the RecSys Challenge 2022's leaderboard.\n",
    "\n",
    "First we load the specific dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "leaderboard_sessions = spark.read.csv(\"dressipi_recsys2022/test_leaderboard_sessions.csv\", header=True)\n",
    "leaderboard_sessions = leaderboard_sessions.withColumn(\"session_id\", leaderboard_sessions[\"session_id\"].cast(\"int\"))\n",
    "leaderboard_sessions = leaderboard_sessions.withColumn(\"item_id\", leaderboard_sessions[\"item_id\"].cast(\"int\"))\n",
    "leaderboard_sessions = leaderboard_sessions.withColumn(\"date\", leaderboard_sessions[\"date\"].cast(\"timestamp\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we compute the values of all the different features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "session_item_date_rdd = leaderboard_sessions.rdd.map(lambda x: (x[\"session_id\"], (x[\"item_id\"], x[\"date\"]))).cache()\n",
    "session_item_rdd = session_item_date_rdd.mapValues(lambda x: (x[0])).cache()\n",
    "session_date_rdd = session_item_date_rdd.mapValues(lambda x: (x[1])).cache()\n",
    "\n",
    "session_item_features_rdd = item_features_rdd.join(leaderboard_sessions.rdd.map(lambda x: (x[\"item_id\"], x[\"session_id\"])))\\\n",
    "                                            .map(lambda x: (x[1][1], x[1][0]))\\\n",
    "                                            .groupByKey()\n",
    "\n",
    "item_features_rdd = item_features.rdd.map(lambda x: (x[\"item_id\"], (x[\"feature_category_id\"], x[\"feature_value_id\"])))\\\n",
    "                                .groupByKey()\\\n",
    "                                .mapValues(lambda x: [(a,b) for a, b in x])\n",
    "\n",
    "session_item_features_rdd = item_features_rdd.join(leaderboard_sessions.rdd.map(lambda x: (x[\"item_id\"], x[\"session_id\"])))\\\n",
    "                                            .map(lambda x: (x[1][1], x[1][0]))\\\n",
    "                                            .groupByKey()\n",
    "\n",
    "\n",
    "month = get_month_feature()\n",
    "season = get_season_feature()\n",
    "day_of_month = get_day_of_month_feature()\n",
    "weekday = get_weekday_feature()\n",
    "weekend = get_weekend_feature()\n",
    "hour = get_hour_feature()\n",
    "day_period = get_day_period_feature()\n",
    "night = get_night_feature()\n",
    "duration = get_duration_feature()\n",
    "average_time = get_average_time_feature()\n",
    "\n",
    "\n",
    "length = get_length_feature()\n",
    "distinct_nb = get_distinct_nb_feature()\n",
    "\n",
    "\n",
    "most_viewed_item = get_most_viewed_item_feature()\n",
    "longest_item = get_longest_item_feature()\n",
    "last_item = get_last_item_feature()\n",
    "\n",
    "\n",
    "categories_nb = get_categories_nb_feature()\n",
    "\n",
    "most_present_category_with_count = session_item_features_rdd.mapValues(get_most_present_category_with_count)\n",
    "\n",
    "most_present_category = get_most_present_category_feature()\n",
    "most_present_category_count = get_most_present_category_count_feature()\n",
    "\n",
    "\n",
    "\n",
    "#repetitive_nb = get_repetitive_nb_feature()\n",
    "#same_category = get_same_category_feature()\n",
    "#diff_category = get_diff_category()\n",
    "\n",
    "#features = [month, season, day_of_month, weekday, weekend, hour, day_period, night, duration, average_time, distinct_nb, repetitive_nb, same_category, diff_category, last_item, most_viewed_item, length, longest_item, categories_nb]\n",
    "features = [month, season, day_of_month, weekday, weekend, hour, day_period, night, duration, average_time, length, distinct_nb, most_viewed_item, longest_item, last_item, categories_nb, most_present_category_with_count, most_present_category, most_present_category_count]\n",
    "\n",
    "test_rdd = features[0].join(features[1])\n",
    "\n",
    "for i in range(2, len(features)):\n",
    "    feature = features[i]\n",
    "    test_rdd = test_rdd.join(feature).mapValues(lambda x: list(x[0])+[x[1]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_rdd.take(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we transform our RDD to a dataframe which can be used by our model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_collector(x):\n",
    "    dico = {}\n",
    "    dico[\"session_id\"] = x[0]\n",
    "    for i in range(len(features_name)):\n",
    "        dico[features_name[i]] = x[1][i]\n",
    "    return dico\n",
    "\n",
    "\n",
    "test_df = test_rdd.map(lambda x: Row(**test_collector(x))).toDF()\n",
    "test_df = test_df.fillna(0)\n",
    "test_df = VectorAssembler(inputCols = features_name, outputCol = \"features\").transform(test_df)\n",
    "test_df = test_df.select([\"features\", \"session_id\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create a new decision tree model that will be trained on the whole dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "decisionTree = DecisionTreeClassifier(maxDepth=2)\n",
    "model = decisionTree.fit(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We get the list of items which should be predicted for the challenge."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "possible_items = candidate_items.rdd.map(lambda x: x[\"item_id\"]).collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally we gather all the results we computed and save them to a CSV file with the structures asked by the RecSys Challenge 2022."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred = model.transform(test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred.show(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "item_to_index = dict()\n",
    "\n",
    "df_iterator = indexed_items_df.rdd.toLocalIterator()\n",
    "\n",
    "for row in df_iterator:\n",
    "    item_to_index[row[\"item_id\"]] = int(row[\"label\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# possible_items_indices = [item_to_index[item] for item in possible_items]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "progressbar = IntProgress(min=0, max=pred.count())\n",
    "display(progressbar)\n",
    "\n",
    "pred_iterator = pred.rdd.toLocalIterator()\n",
    "session_id = 1\n",
    "with open(\"predictions3.csv\", \"w\") as file:\n",
    "    file.write(\"session_id,item_id,rank\\n\")\n",
    "    for row in pred_iterator:\n",
    "        \n",
    "        temp = list(row[\"probability\"])\n",
    "\n",
    "        temp = [(temp[item_to_index[i]], i) for i in possible_items]\n",
    "        temp.sort(reverse=True)\n",
    "        \n",
    "        temp = temp[:100]\n",
    "        for l in range(100):\n",
    "            file.write(f\"{row['session_id']},{temp[l][1]},{l+1}\\n\")\n",
    "        \n",
    "        progressbar.value += 1\n",
    "        session_id += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 3 : Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The model should be trained on the data with the selected features and should returned the predictions required by the competition. For each test session, the model should return 100 candidates items with the highest chance of being purchased. This restricts the differents models that can be used. Therefore the decision tree model was selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first transform our rdd which contains all the data into a dataframe which the classifier can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collector(x):\n",
    "    dico = {}\n",
    "    dico[\"session_id\"] = x[0]\n",
    "    dico[\"item_id\"] = x[1][0]\n",
    "    \n",
    "    for i in range(len(features_name)):\n",
    "        dico[features_name[i]] = x[1][i+1]\n",
    "        \n",
    "    return dico\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "df = BIG_RDD.map(lambda x: Row(**collector(x))).toDF()\n",
    "df = df.fillna(0)\n",
    "\n",
    "# si = StringIndexer(inputCol=\"item_id\", outputCol=\"label\").fit(item_features.select(\"item_id\").distinct())\n",
    "si = StringIndexer(inputCol=\"item_id\", outputCol=\"label\").fit(candidate_items)\n",
    "indexed_items_df = si.transform(candidate_items)\n",
    "df = si.transform(df)\n",
    "\n",
    "df = VectorAssembler(inputCols = features_name, outputCol = \"features\").transform(df)\n",
    "\n",
    "df = df.select([\"features\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionTree = DecisionTreeClassifier(maxDepth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can test the accuracy of our model with cross validation. We chose $K$ = 4 to split the dataset into 75 % training and 25 % testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "grid = ParamGridBuilder().addGrid(decisionTree.featuresCol, [\"features\"]).build()\n",
    "cv = CrossValidator(estimator=decisionTree, estimatorParamMaps=grid, numFolds=2, evaluator=evaluator, parallelism=4)\n",
    "cvModel = cv.fit(df)\n",
    "\n",
    "print(\"Average accuracy:\", cvModel.avgMetrics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy of our model is quite low. This can be explained by the large number of different items in the dataset. Even in the RecSys Challenge 2022, it asked for each session to give 100 items which have the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionTree = DecisionTreeClassifier()\n",
    "model = decisionTree.fit(df)\n",
    "\n",
    "pred = model.transform(df)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\") \n",
    "nbaccuracy = evaluator.evaluate(pred) \n",
    "print(\"Test accuracy = \" + str(nbaccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the predictions for the RecSys Challenge 2022's leaderboard.\n",
    "\n",
    "First we load the specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_sessions = spark.read.csv(\"dressipi_recsys2022/test_leaderboard_sessions.csv\", header=True)\n",
    "leaderboard_sessions = leaderboard_sessions.withColumn(\"session_id\", leaderboard_sessions[\"session_id\"].cast(\"int\"))\n",
    "leaderboard_sessions = leaderboard_sessions.withColumn(\"item_id\", leaderboard_sessions[\"item_id\"].cast(\"int\"))\n",
    "leaderboard_sessions = leaderboard_sessions.withColumn(\"date\", leaderboard_sessions[\"date\"].cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compute the values of all the different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_item_date_rdd = leaderboard_sessions.rdd.map(lambda x: (x[\"session_id\"], (x[\"item_id\"], x[\"date\"]))).cache()\n",
    "session_item_rdd = session_item_date_rdd.mapValues(lambda x: (x[0])).cache()\n",
    "session_date_rdd = session_item_date_rdd.mapValues(lambda x: (x[1])).cache()\n",
    "\n",
    "session_item_features_rdd = item_features_rdd.join(leaderboard_sessions.rdd.map(lambda x: (x[\"item_id\"], x[\"session_id\"])))\\\n",
    "                                            .map(lambda x: (x[1][1], x[1][0]))\\\n",
    "                                            .groupByKey()\n",
    "\n",
    "item_features_rdd = item_features.rdd.map(lambda x: (x[\"item_id\"], (x[\"feature_category_id\"], x[\"feature_value_id\"])))\\\n",
    "                                .groupByKey()\\\n",
    "                                .mapValues(lambda x: [(a,b) for a, b in x])\n",
    "\n",
    "session_item_features_rdd = item_features_rdd.join(leaderboard_sessions.rdd.map(lambda x: (x[\"item_id\"], x[\"session_id\"])))\\\n",
    "                                            .map(lambda x: (x[1][1], x[1][0]))\\\n",
    "                                            .groupByKey()\n",
    "\n",
    "\n",
    "month = get_month_feature()\n",
    "season = get_season_feature()\n",
    "day_of_month = get_day_of_month_feature()\n",
    "weekday = get_weekday_feature()\n",
    "weekend = get_weekend_feature()\n",
    "hour = get_hour_feature()\n",
    "day_period = get_day_period_feature()\n",
    "night = get_night_feature()\n",
    "duration = get_duration_feature()\n",
    "average_time = get_average_time_feature()\n",
    "\n",
    "\n",
    "length = get_length_feature()\n",
    "distinct_nb = get_distinct_nb_feature()\n",
    "\n",
    "\n",
    "most_viewed_item = get_most_viewed_item_feature()\n",
    "longest_item = get_longest_item_feature()\n",
    "last_item = get_last_item_feature()\n",
    "\n",
    "\n",
    "categories_nb = get_categories_nb_feature()\n",
    "\n",
    "most_present_category_with_count = session_item_features_rdd.mapValues(get_most_present_category_with_count)\n",
    "\n",
    "most_present_category = get_most_present_category_feature()\n",
    "most_present_category_count = get_most_present_category_count_feature()\n",
    "\n",
    "\n",
    "\n",
    "#repetitive_nb = get_repetitive_nb_feature()\n",
    "#same_category = get_same_category_feature()\n",
    "#diff_category = get_diff_category()\n",
    "\n",
    "#features = [month, season, day_of_month, weekday, weekend, hour, day_period, night, duration, average_time, distinct_nb, repetitive_nb, same_category, diff_category, last_item, most_viewed_item, length, longest_item, categories_nb]\n",
    "features = [month, season, day_of_month, weekday, weekend, hour, day_period, night, duration, average_time, length, distinct_nb, most_viewed_item, longest_item, last_item, categories_nb, most_present_category_with_count, most_present_category, most_present_category_count]\n",
    "\n",
    "test_rdd = features[0].join(features[1])\n",
    "\n",
    "for i in range(2, len(features)):\n",
    "    feature = features[i]\n",
    "    test_rdd = test_rdd.join(feature).mapValues(lambda x: list(x[0])+[x[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we transform our RDD to a dataframe which can be used by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_collector(x):\n",
    "    dico = {}\n",
    "    dico[\"session_id\"] = x[0]\n",
    "    for i in range(len(features_name)):\n",
    "        dico[features_name[i]] = x[1][i]\n",
    "    return dico\n",
    "\n",
    "\n",
    "test_df = test_rdd.map(lambda x: Row(**test_collector(x))).toDF()\n",
    "test_df = test_df.fillna(0)\n",
    "test_df = VectorAssembler(inputCols = features_name, outputCol = \"features\").transform(test_df)\n",
    "test_df = test_df.select([\"features\", \"session_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new decision tree model that will be trained on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionTree = DecisionTreeClassifier(maxDepth=2)\n",
    "model = decisionTree.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the list of items which should be predicted for the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_items = candidate_items.rdd.map(lambda x: x[\"item_id\"]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we gather all the results we computed and save them to a CSV file with the structures asked by the RecSys Challenge 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_to_index = dict()\n",
    "\n",
    "df_iterator = indexed_items_df.rdd.toLocalIterator()\n",
    "\n",
    "for row in df_iterator:\n",
    "    item_to_index[row[\"item_id\"]] = int(row[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible_items_indices = [item_to_index[item] for item in possible_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progressbar = IntProgress(min=0, max=pred.count())\n",
    "display(progressbar)\n",
    "\n",
    "pred_iterator = pred.rdd.toLocalIterator()\n",
    "session_id = 1\n",
    "with open(\"predictions3.csv\", \"w\") as file:\n",
    "    file.write(\"session_id,item_id,rank\\n\")\n",
    "    for row in pred_iterator:\n",
    "        \n",
    "        temp = list(row[\"probability\"])\n",
    "\n",
    "        temp = [(temp[item_to_index[i]], i) for i in possible_items]\n",
    "        temp.sort(reverse=True)\n",
    "        \n",
    "        temp = temp[:100]\n",
    "        for l in range(100):\n",
    "            file.write(f\"{row['session_id']},{temp[l][1]},{l+1}\\n\")\n",
    "        \n",
    "        progressbar.value += 1\n",
    "        session_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}